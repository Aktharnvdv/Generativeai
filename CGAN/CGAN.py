# -*- coding: utf-8 -*-
"""CGAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16h46XAz3ISp1GXd4MA0LyrWjLcwo_cW2
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec

!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz
!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz
!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/tlok-images-idx3-ubyte.gz
!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/tlok-labels-idx1-ubyte.gz

!mkdr MNIST_Fashion

!cp *.gz MNIST_Fashion/
from tensorflow.examples.tutorials.mnist import input_data
mnist=input_data.read_data_sets("MNIST_Fashion/",one_hot=True)

print(mnist.train.images.shape)
print(mnist.test.images.shape)
print(mnist.train.labels.shape)
print(mnist.test.labels.shape)

plt.figure(figsize=(1,1))
sample_image=mnist.train.next_batch(1)[0]
print(sample_image[0])
sample_image=sample_image.reshape([28,28])
plt.imshow(sample_image,cmap="Greys")

learning_rate=0.0002
batch_size=128
epochs=10000
image_dim=784
Y_dimension=10
gen_hidd_dim=256
disc_hidd_dim=256
z_noise_dim=100
def xavier_init(shape):
  return tf.random_normal(shape=shape,stddev=1./tf.sqrt(shape[0]/2.0))

weights={"disc_H":tf.Variable(xavier_init([image_dim+Y_dimension,disc_hidd_dim])),
         "disc_final":tf.Variable(xavier_init([disc_hidd_dim,1])),
         "gen_H":tf.Variable(xavier_init([z_noise_dim+Y_dimension,gen_hidd_dim])),
         "gen_final":tf.Variable(xavier_init([gen_hidd_dim,image_dim]))
        }
bias={"disc_H":tf.Variable(xavier_init([disc_hidd_dim])),
      "disc_final":tf.Variable(xavier_init([1])),
      "gen_H":tf.Variable(xavier_init([gen_hidd_dim])),
      "gen_final":tf.Variable(xavier_init([image_dim]))
       }

z_input=tf.placeholder(tf.float32,shape=[None,z_noise_dim],name="input_noise")
Y_input=tf.placeholder(tf.float32,shape=[None,Y_dimension],name="Labels")
x_input=tf.placeholder(tf.float32,shape=[None,image_dim],name="real_input")

def Discriminator(x,y):
  inputs=tf.concat(axis=1,values=[x,y])
  hidden_layer=tf.nn.relu(tf.add(tf.matmul(inputs,weights["disc_H"]),bias["disc_H"]))
  final_layer=tf.add(tf.matmul(hidden_layer,weights["disc_final"]),bias["disc_final"])
  disc_output=tf.nn.sigmoid(final_layer)
  return final_layer,disc_output

def Generator(x,y):
  inputs=tf.concat(axis=1,values=[x,y])
  hidden_layer=tf.nn.relu(tf.add(tf.matmul(inputs,weights['gen_H']),bias["gen_H"]))
  final_layer=tf.add(tf.matmul(hidden_layer,weights["gen_final"]),bias["gen_final"])
  gen_output=tf.nn.sigmoid(final_layer)
  return gen_output

output_Gen=Generator(z_input,Y_input)
real_output1_Disc,real_output_Disc=Discriminator(x_input,Y_input)
fake_output1_Disc,fake_output_Disc=Discriminator(output_Gen,Y_input)

Discriminator_Loss=-tf.reduce_mean(tf.log(real_output_Disc+.0001)+tf.log(1.-fake_output_Disc+.0001))
Generator_Loss=-tf.reduce_mean(tf.log(fake_output_Disc+.0001))
Disc_loss_total=tf.summary.scalar("Disc_Total_loss",Discriminator_Loss)
Gen_loss_total=tf.summary.scalar("Gen_Loss",Generator_Loss)

Disc_real_loss=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=real_output1_Disc,labels=tf.ones_like(real_output1_Disc)))
Disc_fake_loss=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_output1_Disc,labels=tf.zeros_like(fake_output1_Disc)))
Discriminator_loss=Disc_real_loss + Disc_fake_loss

Generator_Loss=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_output1_Disc,labels=tf.ones_like(fake_output1_Disc)))

Disc_loss_real_summary=tf.summary.scalar("Disc_loss_real",Disc_real_loss)
Disc_loss_fake_summary=tf.summary.scalar("disc_loss_fake",Disc_fake_loss)
Disc_loss_summary=tf.summary.scalar("Disc_Total_loss",Discriminator_Loss)

Disc_loss_total=tf.summary.merge([Disc_loss_real_summary,Disc_loss_fake_summary,Disc_loss_summary])
Gen_loss_total=tf.summary.scalar("Gens_Loss",Generator_Loss)

Generator_var=[weights["gen_H"],weights["gen_final"],bias["gen_H"],bias["gen_final"]]
Discriminator_var=[weights["disc_H"],weights["disc_final"],bias["disc_H"],bias["disc_final"]]
Discriminator_optimize=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(Discriminator_Loss,var_list=Discriminator_var)
Generator_optimize=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(Generator_Loss,var_list=Generator_var)

init=tf.global_variables_initializer()
sess=tf.Session()
sess.run(init)
writer=tf.summary.FileWriter("./log",sess.graph)

for epoch in range(epochs):
  x_batch,Y_label=mnist.train.next_batch(batch_size)
  z_noise=np.random.uniform(-1.,1.,size=[batch_size,z_noise_dim])
  _,Disc_loss_epoch=sess.run([Discriminator_optimize,Discriminator_Loss],feed_dict={x_input:x_batch,Y_input:Y_label,z_input:z_noise})
  _,Gen_loss_epoch=sess.run([Generator_optimize,Generator_Loss],feed_dict={z_input:z_noise,Y_input:Y_label})


  summary_Disc_Loss=sess.run(Disc_loss_total,feed_dict={x_input:x_batch,z_input:z_noise,Y_input:Y_label})
  writer.add_summary(summary_Disc_Loss,epoch)
  summary_Gen_Loss=sess.run(Gen_loss_total,feed_dict={z_input:z_noise,Y_input:Y_label})
  writer.add_summary(summary_Gen_Loss,epoch)
  if epoch % 2000==0:
    print("scope:{0},  Generator Loss:{1},  Discriminator Loss:{2}".format(epoch,Gen_loss_epoch,Disc_loss_epoch))

def generated_plot(samples):
  fig=plt.figure(figsize=(4,4))
  gn=gridspec.GridSpec(4,4)
  gn.update(wspace=.05,hspace=.05)
  for i,sample in enumerate(samples):
    ax=plt.subplot(gn[i])
    plt.axis("off")
    ax.set_xticks([])
    ax.set_yticks([])
    ax.set_aspect("equal")
    plt.imshow(sample.reshape(28,28),cmap="gray")
  return fig

def create(inp):
  feature_map={
      "t-shirt":0,
      "trouser":1,
      "pullover":2,
      "dress":3,
      "coat":4,
      "sandal":5,
      "shirt":6,
      "sneaker":7,
      "bag":8,
      "ankle boot":9
  }
  samples=16
  z_noise=np.random.uniform(-1.,1.,size=[samples,z_noise_dim])
  Y_label=np.zeros(shape=[samples,Y_dimension])
  Y_label[:,feature_map[inp]]=1
  generated_samples=sess.run(output_Gen,feed_dict={z_input:z_noise,Y_input:Y_label})
  generated_plot(generated_samples)

create("pullover")
