{
<<<<<<< HEAD
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y4A9SyiB9uFw",
    "outputId": "77a31eaa-e033-4e1a-d329-b19240ccec9b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchfile\n",
      "  Downloading torchfile-0.1.0.tar.gz (5.2 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting data\n",
      "  Downloading data-0.4.tar.gz (7.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/lib/python3/dist-packages (from data) (1.16.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from data) (5.1.1)\n",
      "Collecting funcsigs (from data)\n",
      "  Downloading funcsigs-1.0.2-py2.py3-none-any.whl (17 kB)\n",
      "Building wheels for collected packages: torchfile, data\n",
      "  Building wheel for torchfile (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for torchfile: filename=torchfile-0.1.0-py3-none-any.whl size=5694 sha256=592e0e190c008a6cbf75f0032b8e8701fa9d78ffca79f55dc81a945c230284f0\n",
      "  Stored in directory: /root/.cache/pip/wheels/c7/e9/87/1c51daf8e468d5c14931f8ac3344880f903ba96b063675cac2\n",
      "  Building wheel for data (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for data: filename=data-0.4-py3-none-any.whl size=7227 sha256=5396eea152c9b6bd7704279a747adc07e6d4dcb90e60c2bc9e5ae07cc5cee825\n",
      "  Stored in directory: /root/.cache/pip/wheels/d0/e8/fa/e253c256048ea58d99a8abb5e751abb6a838af6f12887b5418\n",
      "Successfully built torchfile data\n",
      "Installing collected packages: torchfile, funcsigs, data\n",
      "Successfully installed data-0.4 funcsigs-1.0.2 torchfile-0.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Cloning into 'Text-to-Photo-realistic-Image-Synthesis-with-Stacked-Generative-Adversarial-Networks'...\n",
      "remote: Enumerating objects: 58, done.\u001b[K\n",
      "remote: Counting objects: 100% (58/58), done.\u001b[K\n",
      "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
      "remote: Total 58 (delta 22), reused 45 (delta 14), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (58/58), 16.82 KiB | 8.41 MiB/s, done.\n",
      "Resolving deltas: 100% (22/22), done.\n",
      "/workspace/Text-to-Photo-realistic-Image-Synthesis-with-Stacked-Generative-Adversarial-Networks/code\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "!pip install torchfile data scipy\n",
    "!git clone https://github.com/zeusm9/Text-to-Photo-realistic-Image-Synthesis-with-Stacked-Generative-Adversarial-Networks.git\n",
    "%cd Text-to-Photo-realistic-Image-Synthesis-with-Stacked-Generative-Adversarial-Networks/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "cje9tqEzm3OI"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch, datetime, dateutil.tz\n",
    "import torchvision.transforms as transforms\n",
    "import config as cfg\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import TextDataset\n",
    "import torch.utils.data as data\n",
    "import os.path\n",
    "import PIL\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import config as cfg\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torchfile\n",
    "import numpy as np\n",
    "import pickle\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from utils import mkdir_p\n",
    "from torch.autograd import Variable\n",
    "from torchvision.models.inception import inception_v3\n",
    "from scipy.stats import entropy\n",
    "from torch.nn import functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import config as cfg\n",
    "from utils import weights_init, discriminator_loss, generator_loss, KL_loss, save_img_results, save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stage1_G(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Stage1_G, self).__init__()\n",
    "        self.gf_dim = cfg.GAN_GF_DIM * 8\n",
    "        self.ef_dim = cfg.GAN_CONDITION_DIM\n",
    "        self.z_dim = cfg.Z_DIM\n",
    "        self.define_module()\n",
    "\n",
    "    def define_module(self):\n",
    "        ninput = self.z_dim + self.ef_dim\n",
    "        ngf = self.gf_dim\n",
    "\n",
    "        self.ca_net = Ca_Net()\n",
    "\n",
    "        # ngf x 4 x 4\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(12928, ngf * 4 * 4, bias=False),\n",
    "            nn.BatchNorm1d(ngf * 4 * 4),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        # ngf x 4 x 4 -> ngf/2 x 8 x 8\n",
    "        self.upsample1 = upBlock(ngf, ngf // 2)\n",
    "        # -> ngf/4 x 16 x 16\n",
    "        self.upsample2 = upBlock(ngf // 2, ngf // 4)\n",
    "        # -> ngf/8 x 32 x 32\n",
    "        self.upsample3 = upBlock(ngf // 4, ngf // 8)\n",
    "        # -> ngf/16 x 64 x 64\n",
    "        self.upsample4 = upBlock(ngf // 8, ngf // 16)\n",
    "        # -> 3 x 64 x 64\n",
    "        self.img = nn.Sequential(conv3x3(ngf // 16, 3), nn.Tanh())\n",
    "\n",
    "    def forward(self, text_embedding, noise):\n",
    "        \n",
    "        c_code, mu, logvar = self.ca_net(text_embedding)\n",
    "        noise = noise.unsqueeze(1).expand(-1, c_code.size(1), -1)\n",
    "        z_c_code = torch.cat((noise, c_code.unsqueeze(2)), 2)\n",
    "        z_c_code_flat = z_c_code.view(-1, z_c_code.size(1) * z_c_code.size(2))\n",
    "        print(\"Size of z_c_code_flat:\", z_c_code_flat.size())\n",
    "        print(\"Size of fc weight:\", self.fc[0].weight.size())\n",
    "        h_code = self.fc(z_c_code_flat)\n",
    "\n",
    "        h_code = h_code.view(-1, self.gf_dim, 4, 4)\n",
    "        h_code = self.upsample1(h_code)\n",
    "        h_code = self.upsample2(h_code)\n",
    "        h_code = self.upsample3(h_code)\n",
    "        h_code = self.upsample4(h_code)\n",
    "\n",
    "        fake_img = self.img(h_code)\n",
    "        return None, fake_img, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stage1_D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Stage1_D, self).__init__()\n",
    "        self.df_dim = cfg.GAN_DF_DIM\n",
    "        self.ef_dim = cfg.GAN_CONDITION_DIM\n",
    "        self.define_module()\n",
    "\n",
    "    def define_module(self):\n",
    "        ndf, nef = self.df_dim, self.ef_dim\n",
    "        self.encode_img = nn.Sequential(\n",
    "            nn.Conv2d(3, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            # state size (ndf * 8) x 4 x 4)\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        self.get_cond_logits = D_Logits(ndf, nef)\n",
    "        self.get_uncond_logits = None\n",
    "\n",
    "    def forward(self, image):\n",
    "        img_embedding = self.encode_img(image)\n",
    "\n",
    "        return img_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stage2_G(nn.Module):\n",
    "    def __init__(self, Stage1_G):\n",
    "        super(Stage2_G, self).__init__()\n",
    "        self.gf_dim = cfg.GAN_GF_DIM\n",
    "        self.ef_dim = cfg.GAN_CONDITION_DIM\n",
    "        self.z_dim = cfg.Z_DIM\n",
    "        self.Stage1_G = Stage1_G\n",
    "\n",
    "        for param in self.Stage1_G.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.define_module()\n",
    "\n",
    "    def _make_layer(self, block, channel_num):\n",
    "        layers = []\n",
    "        for i in range(cfg.GAN_R_NUM):\n",
    "            layers.append(block(channel_num))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def define_module(self):\n",
    "        ngf = self.gf_dim\n",
    "        self.ca_net = Ca_Net()\n",
    "        self.encoder = nn.Sequential(\n",
    "            conv3x3(3, ngf),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(ngf, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(ngf * 2, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True))\n",
    "        self.hr_joint = nn.Sequential(\n",
    "            conv3x3(self.ef_dim + ngf * 4, ngf * 4),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.residual = self._make_layer(ResBlock, ngf * 4)\n",
    "        self.upsample1 = upBlock(ngf * 4, ngf * 2)\n",
    "        self.upsample2 = upBlock(ngf * 2, ngf)\n",
    "        self.upsample3 = upBlock(ngf, ngf // 2)\n",
    "        self.upsample4 = upBlock(ngf // 2, ngf // 4)\n",
    "        self.img = nn.Sequential(\n",
    "            conv3x3(ngf // 4, 3),\n",
    "            nn.Tanh())\n",
    "\n",
    "    def forward(self, text_embedding, noise):\n",
    "        _, stage1_img, _, _ = self.Stage1_G(text_embedding, noise)\n",
    "        stage1_img = stage1_img.detach()\n",
    "        encoded_img = self.encoder(stage1_img)\n",
    "\n",
    "        c_code, mu, logvar = self.ca_net(text_embedding)\n",
    "        c_code = c_code.view(-1, self.ef_dim, 1, 1)\n",
    "        c_code = c_code.repeat(1, 1, 16, 16)\n",
    "        i_c_code = torch.cat([encoded_img, c_code], 1)\n",
    "        h_code = self.hr_joint(i_c_code)\n",
    "        h_code = self.residual(h_code)\n",
    "\n",
    "        h_code = self.upsample1(h_code)\n",
    "        h_code = self.upsample2(h_code)\n",
    "        h_code = self.upsample3(h_code)\n",
    "        h_code = self.upsample4(h_code)\n",
    "\n",
    "        fake_img = self.img(h_code)\n",
    "        return stage1_img, fake_img, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stage2_D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Stage2_D, self).__init__()\n",
    "        self.df_dim = cfg.GAN_DF_DIM\n",
    "        self.ef_dim = cfg.GAN_CONDITION_DIM\n",
    "        self.define_module()\n",
    "\n",
    "    def define_module(self):\n",
    "        ndf, nef = self.df_dim, self.ef_dim\n",
    "        self.encode_img = nn.Sequential(\n",
    "            nn.Conv2d(3, ndf, 4, 2, 1, bias=False),  # 128 * 128 * ndf\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # 64 * 64 * ndf * 2\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # 32 * 32 * ndf * 4\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # 16 * 16 * ndf * 8\n",
    "            nn.Conv2d(ndf * 8, ndf * 16, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # 8 * 8 * ndf * 16\n",
    "            nn.Conv2d(ndf * 16, ndf * 32, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # 4 * 4 * ndf * 32\n",
    "            conv3x3(ndf * 32, ndf * 16),\n",
    "            nn.BatchNorm2d(ndf * 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # 4 * 4 * ndf * 16\n",
    "            conv3x3(ndf * 16, ndf * 8),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True)  # 4 * 4 * ndf * 8\n",
    "        )\n",
    "\n",
    "        self.get_cond_logits = D_GET_LOGITS(ndf, nef, bcondition=True)\n",
    "        self.get_uncond_logits = D_GET_LOGITS(ndf, nef, bcondition=False)\n",
    "\n",
    "    def forward(self, image):\n",
    "        img_embedding = self.encode_img(image)\n",
    "\n",
    "        return img_embedding\n",
    "\n",
    "\n",
    "class Ca_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Ca_Net, self).__init__()\n",
    "        self.t_dim = cfg.TEXT_DIMENSION\n",
    "        self.c_dim = cfg.GAN_CONDITION_DIM\n",
    "        self.fc = nn.Linear(self.t_dim, self.c_dim * 2, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def encode(self, text_embedding):\n",
    "        x = self.relu(self.fc(text_embedding))\n",
    "        mu = x[:, :self.c_dim]\n",
    "        logvar = x[:, self.c_dim:]\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        if cfg.CUDA:\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def forward(self, text_embedding):\n",
    "        mu, logvar = self.encode(text_embedding)\n",
    "        c_code = self.reparametrize(mu, logvar)\n",
    "        return c_code, mu, logvar\n",
    "\n",
    "\n",
    "class D_Logits(nn.Module):\n",
    "    def __init__(self, ndf, nef, bcondition=True):\n",
    "        super(D_Logits, self).__init__()\n",
    "        self.df_dim = ndf\n",
    "        self.ef_dim = nef\n",
    "        self.bcondition = bcondition\n",
    "\n",
    "        if bcondition:\n",
    "            self.outlogits = nn.Sequential(\n",
    "                conv3x3(ndf * 8 + nef, ndf * 8),\n",
    "                nn.BatchNorm2d(ndf * 8),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Conv2d(ndf * 8, 1, kernel_size=4, stride=4),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        else:\n",
    "            self.outlogits = nn.Sequential(\n",
    "                nn.Conv2d(ndf * 8, 1, kernel_size=4, stride=4),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "    def forward(self, h_code, c_code=None):\n",
    "        if self.bcondition and c_code is not None:\n",
    "            c_code = c_code.view(-1, self.ef_dim, 1, 1)\n",
    "            c_code = c_code.repeat(1, 1, 4, 4)\n",
    "            h_c_code = torch.cat((h_code, c_code), 1)\n",
    "        else:\n",
    "            h_c_code = h_code\n",
    "        output = self.outlogits(h_c_code)\n",
    "        return output.view(-1)\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channel_num):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            conv3x3(channel_num, channel_num),\n",
    "            nn.BatchNorm2d(channel_num),\n",
    "            nn.ReLU(True),\n",
    "            conv3x3(channel_num, channel_num),\n",
    "            nn.BatchNorm2d(channel_num))\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.block(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class D_GET_LOGITS(nn.Module):\n",
    "    def __init__(self, ndf, nef, bcondition=True):\n",
    "        super(D_GET_LOGITS, self).__init__()\n",
    "        self.df_dim = ndf\n",
    "        self.ef_dim = nef\n",
    "        self.bcondition = bcondition\n",
    "        if bcondition:\n",
    "            self.outlogits = nn.Sequential(\n",
    "                conv3x3(ndf * 8 + nef, ndf * 8),\n",
    "                nn.BatchNorm2d(ndf * 8),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Conv2d(ndf * 8, 1, kernel_size=4, stride=4),\n",
    "                nn.Sigmoid())\n",
    "        else:\n",
    "            self.outlogits = nn.Sequential(\n",
    "                nn.Conv2d(ndf * 8, 1, kernel_size=4, stride=4),\n",
    "                nn.Sigmoid())\n",
    "\n",
    "    def forward(self, h_code, c_code=None):\n",
    "        # conditioning output\n",
    "        if self.bcondition and c_code is not None:\n",
    "            c_code = c_code.view(-1, self.ef_dim, 1, 1)\n",
    "            c_code = c_code.repeat(1, 1, 4, 4)\n",
    "            # state size (ngf+egf) x 4 x 4\n",
    "            h_c_code = torch.cat((h_code, c_code), 1)\n",
    "        else:\n",
    "            h_c_code = h_code\n",
    "\n",
    "        output = self.outlogits(h_c_code)\n",
    "        return output.view(-1)\n",
    "\n",
    "\n",
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "def upBlock(in_channels, out_channels):\n",
    "    block = nn.Sequential(\n",
    "        nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "        conv3x3(in_channels, out_channels),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(True)\n",
    "    )\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "RYpjUGMbzwGZ"
   },
   "outputs": [],
   "source": [
    "class GANTrainer(object):\n",
    "    def __init__(self, output_dir):\n",
    "        if cfg.TRAIN_FLAG:\n",
    "            self.model_dir = os.path.join(output_dir, 'Model')\n",
    "            self.image_dir = os.path.join(output_dir, 'Image')\n",
    "            self.log_dir = os.path.join(output_dir, 'Log')\n",
    "\n",
    "        self.gpus = [0]\n",
    "        self.max_epoch = cfg.TRAIN_MAX_EPOCH\n",
    "        self.snapshot_interval = cfg.TRAIN_SNAPSHOT_INTERVAL\n",
    "        self.batch_size = cfg.TRAIN_BATCH_SIZE\n",
    "        torch.cuda.device(0)\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "    def get_imgs(self):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(32),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        ])\n",
    "        imgs = []\n",
    "        for i in range(0, 2911):\n",
    "            img = Image.open(\"../data/birds/models/netG_epoch_360/\" + str(i) + \".png\").convert('RGB')\n",
    "            load_size = int(cfg.IMSIZE * 76 / 64)\n",
    "            img = img.resize((load_size, load_size), PIL.Image.BILINEAR)\n",
    "            img = transform(img)\n",
    "            img = np.array(img)\n",
    "            imgs.append(img)\n",
    "        return imgs\n",
    "\n",
    "    def load_network_stageI(self):\n",
    "        netG = Stage1_G()\n",
    "        netG.apply(weights_init)\n",
    "        print(netG)\n",
    "        netD = Stage1_D()\n",
    "        netD.apply(weights_init)\n",
    "        print(netD)\n",
    "\n",
    "        if cfg.NET_G != '':\n",
    "            state_dict = torch.load(cfg.NET_G, map_location=lambda storage, loc: storage)\n",
    "            netG.load_state_dict(state_dict)\n",
    "            print('Load from: ', cfg.NET_G)\n",
    "        if cfg.NET_D != '':\n",
    "            state_dict = torch.load(cfg.NET_D, map_location=lambda storage, loc: storage)\n",
    "            netD.load_state_dict(state_dict)\n",
    "            print('Load from: ', cfg.NET_D)\n",
    "\n",
    "        if cfg.CUDA:\n",
    "            netG.cuda()\n",
    "            netD.cuda()\n",
    "        return netG, netD\n",
    "\n",
    "    def load_network_stageII(self):\n",
    "        from model import Stage1_G, Stage2_G, Stage2_D\n",
    "        Stage1_G = Stage1_G()\n",
    "        netG = Stage2_G(Stage1_G)\n",
    "        netG.apply(weights_init)\n",
    "        print(netG)\n",
    "        if cfg.NET_G != '':\n",
    "            state_dict = torch.load(cfg.NET_G,\n",
    "                                    map_location=lambda storage, loc: storage)\n",
    "            netG.load_state_dict(state_dict)\n",
    "            print('Load from: ', cfg.NET_G)\n",
    "        elif cfg.STAGE1_G != '':\n",
    "            state_dict = torch.load(cfg.STAGE1_G,\n",
    "                                    map_location=lambda storage, loc: storage)\n",
    "            netG.Stage1_G.load_state_dict(state_dict)\n",
    "            print('Load from: ', cfg.STAGE1_G)\n",
    "        else:\n",
    "            print(\"Please give the Stage1_G path\")\n",
    "            return\n",
    "\n",
    "        netD = Stage2_D()\n",
    "        netD.apply(weights_init)\n",
    "        if cfg.NET_D != '':\n",
    "            state_dict = \\\n",
    "                torch.load(cfg.NET_D,\n",
    "                           map_location=lambda storage, loc: storage)\n",
    "            netD.load_state_dict(state_dict)\n",
    "            print('Load from: ', cfg.NET_D)\n",
    "        print(netD)\n",
    "\n",
    "        if cfg.CUDA:\n",
    "            netG.cuda()\n",
    "            netD.cuda()\n",
    "        return netG, netD\n",
    "\n",
    "    def train(self, data_loader, stage=1):\n",
    "        if stage == 1:\n",
    "            netG, netD = self.load_network_stageI()\n",
    "        else:\n",
    "            netG, netD = self.load_network_stageII()\n",
    "\n",
    "        nz = cfg.Z_DIM\n",
    "        batch_size = self.batch_size\n",
    "        noise = Variable(torch.FloatTensor(batch_size, nz))\n",
    "        fixed_noise = Variable(torch.FloatTensor(batch_size, nz).normal_(0, 1), requires_grad=True)\n",
    "        real_labels = Variable(torch.FloatTensor(batch_size).fill_(1))\n",
    "        fake_labels = Variable(torch.FloatTensor(batch_size).fill_(0))\n",
    "\n",
    "        if cfg.CUDA:\n",
    "            noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
    "            real_labels, fake_labels = real_labels.cuda(), fake_labels.cuda()\n",
    "\n",
    "        generator_lr = cfg.TRAIN_GENERATOR_LR\n",
    "        discriminator_lr = cfg.TRAIN_DISCRIMINATOR_LR\n",
    "        lr_decay_step = cfg.TRAIN_LR_DECAY_EPOCH\n",
    "        optimizerD = optim.Adam(netD.parameters(), lr=cfg.TRAIN_DISCRIMINATOR_LR, betas=(0.5, 0.999))\n",
    "\n",
    "        netG_para = []\n",
    "        for p in netG.parameters():\n",
    "            if p.requires_grad:\n",
    "                netG_para.append(p)\n",
    "        optimizerG = optim.Adam(netG_para, lr=cfg.TRAIN_GENERATOR_LR, betas=(0.5, 0.999))\n",
    "\n",
    "        count = 0\n",
    "        for epoch in range(self.max_epoch):\n",
    "            start_t = time.time()\n",
    "            if epoch % lr_decay_step == 0 and epoch > 0:\n",
    "                generator_lr *= 0.5\n",
    "                for param_group in optimizerG.param_groups:\n",
    "                    param_group['lr'] = generator_lr\n",
    "                discriminator_lr *= 0.5\n",
    "                for param_group in optimizerD.param_groups:\n",
    "                    param_group['lr'] = discriminator_lr\n",
    "            for i, data in enumerate(data_loader, 0):\n",
    "                # Prepare training data\n",
    "                real_img_cpu, txt_embedding = data\n",
    "                real_imgs = Variable(real_img_cpu)\n",
    "                txt_embedding = Variable(txt_embedding)\n",
    "                if cfg.CUDA:\n",
    "                    real_imgs = real_imgs.cuda()\n",
    "                    txt_embedding = txt_embedding.cuda()\n",
    "                # Generate fake images\n",
    "                noise.data.normal_(0, 1)\n",
    "                inputs = (txt_embedding, noise)\n",
    "                _, fake_imgs, mu, logvar = nn.parallel.data_parallel(netG, inputs, self.gpus)\n",
    "\n",
    "                netD.zero_grad()\n",
    "                errD, errD_real, errD_wrong, errD_fake = discriminator_loss(netD, real_imgs, fake_imgs,\n",
    "                                                                            real_labels, fake_labels, mu,\n",
    "                                                                            self.gpus)\n",
    "                errD.backward()\n",
    "                optimizerD.step()\n",
    "\n",
    "                netG.zero_grad()\n",
    "                errG = generator_loss(netD, fake_imgs,\n",
    "                                      real_labels, mu, self.gpus)\n",
    "                kl_loss = KL_loss(mu, logvar)\n",
    "                errG_total = errG + kl_loss * cfg.TRAIN_COEFF_KL\n",
    "                errG_total.backward()\n",
    "                optimizerG.step()\n",
    "\n",
    "                count = count + 1\n",
    "\n",
    "                if i % 100 == 0:\n",
    "\n",
    "                    # save the image result for each epoch\n",
    "                    inputs = (txt_embedding, fixed_noise)\n",
    "                    lr_fake, fake, _, _ = \\\n",
    "                        nn.parallel.data_parallel(netG, inputs, self.gpus)\n",
    "                    save_img_results(real_img_cpu, fake, epoch, self.image_dir)\n",
    "                    if lr_fake is not None:\n",
    "                        save_img_results(None, lr_fake, epoch, self.image_dir)\n",
    "                end_t = time.time()\n",
    "                print('''[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f Loss_KL: %.4f\n",
    "                                     Loss_real: %.4f Loss_wrong:%.4f Loss_fake %.4f\n",
    "                                     Total Time: %.2fsec\n",
    "                                  '''\n",
    "                      % (epoch, self.max_epoch, i, len(data_loader),\n",
    "                         errD.data, errG.data, kl_loss.data,\n",
    "                         errD_real, errD_wrong, errD_fake, (end_t - start_t)))\n",
    "                if epoch % self.snapshot_interval == 0:\n",
    "                    save_model(netG, netD, epoch, self.model_dir)\n",
    "                #\n",
    "            save_model(netG, netD, self.max_epoch, self.model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "faAF7hz2xwUJ"
   },
   "outputs": [],
   "source": [
    "class RandomDataset(Dataset):\n",
    "    def __init__(self, data_size, embedding_size):\n",
    "        self.data_size = data_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.random_images = torch.randn(self.data_size, 3, 64, 64)\n",
    "        self.random_embeddings = torch.randn(self.data_size,\n",
    "                                             self.embedding_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.random_images[idx],self.random_embeddings[idx])\n",
    "\n",
    "random_dataset = RandomDataset(data_size=1000, embedding_size=1024)\n",
    "batch_size = 32\n",
    "data_loader = DataLoader(dataset=random_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hGgzX6xkxt1r",
    "outputId": "64773ad6-2565-453b-acd8-eec1b8e96ce9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage1_G(\n",
      "  (ca_net): Ca_Net(\n",
      "    (fc): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=228, out_features=24576, bias=False)\n",
      "    (1): BatchNorm1d(24576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (upsample1): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='nearest')\n",
      "    (1): Conv2d(1536, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "  )\n",
      "  (upsample2): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='nearest')\n",
      "    (1): Conv2d(768, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "  )\n",
      "  (upsample3): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='nearest')\n",
      "    (1): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "  )\n",
      "  (upsample4): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='nearest')\n",
      "    (1): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "  )\n",
      "  (img): Sequential(\n",
      "    (0): Conv2d(96, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): Tanh()\n",
      "  )\n",
      ")\n",
      "Stage1_D(\n",
      "  (encode_img): Sequential(\n",
      "    (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(96, 192, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(192, 384, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(384, 768, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (get_cond_logits): D_Logits(\n",
      "    (outlogits): Sequential(\n",
      "      (0): Conv2d(896, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      (3): Conv2d(768, 1, kernel_size=(4, 4), stride=(4, 4))\n",
      "      (4): Sigmoid()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Size of z_c_code_flat: torch.Size([32, 12928])\n",
      "Size of fc weight: torch.Size([24576, 228])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x12928 and 228x24576)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/workspace\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m algo \u001b[38;5;241m=\u001b[39m GANTrainer(output_dir)\n\u001b[0;32m----> 3\u001b[0m \u001b[43malgo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[100], line 137\u001b[0m, in \u001b[0;36mGANTrainer.train\u001b[0;34m(self, data_loader, stage)\u001b[0m\n\u001b[1;32m    135\u001b[0m noise\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnormal_(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    136\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (txt_embedding, noise)\n\u001b[0;32m--> 137\u001b[0m _, fake_imgs, mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m netD\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    140\u001b[0m errD, errD_real, errD_wrong, errD_fake \u001b[38;5;241m=\u001b[39m discriminator_loss(netD, real_imgs, fake_imgs,\n\u001b[1;32m    141\u001b[0m                                                             real_labels, fake_labels, mu,\n\u001b[1;32m    142\u001b[0m                                                             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpus)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/data_parallel.py:265\u001b[0m, in \u001b[0;36mdata_parallel\u001b[0;34m(module, inputs, device_ids, output_device, dim, module_kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m module_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(device_ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m used_device_ids \u001b[38;5;241m=\u001b[39m device_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)]\n\u001b[1;32m    267\u001b[0m replicas \u001b[38;5;241m=\u001b[39m replicate(module, used_device_ids)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[96], line 41\u001b[0m, in \u001b[0;36mStage1_G.forward\u001b[0;34m(self, text_embedding, noise)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize of z_c_code_flat:\u001b[39m\u001b[38;5;124m\"\u001b[39m, z_c_code_flat\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize of fc weight:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m---> 41\u001b[0m h_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz_c_code_flat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m h_code \u001b[38;5;241m=\u001b[39m h_code\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgf_dim, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     44\u001b[0m h_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsample1(h_code)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x12928 and 228x24576)"
     ]
    }
   ],
   "source": [
    "output_dir = \"/workspace\"\n",
    "algo = GANTrainer(output_dir)\n",
    "algo.train(data_loader, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
=======
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torchfile data\n",
        "!git clone https://github.com/zeusm9/Text-to-Photo-realistic-Image-Synthesis-with-Stacked-Generative-Adversarial-Networks.git\n",
        "%cd Text-to-Photo-realistic-Image-Synthesis-with-Stacked-Generative-Adversarial-Networks/code"
      ],
      "metadata": {
        "id": "y4A9SyiB9uFw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77a31eaa-e033-4e1a-d329-b19240ccec9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchfile in /usr/local/lib/python3.10/dist-packages (0.1.0)\n",
            "Requirement already satisfied: data in /usr/local/lib/python3.10/dist-packages (0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from data) (1.16.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from data) (4.4.2)\n",
            "Requirement already satisfied: funcsigs in /usr/local/lib/python3.10/dist-packages (from data) (1.0.2)\n",
            "Cloning into 'Text-to-Photo-realistic-Image-Synthesis-with-Stacked-Generative-Adversarial-Networks'...\n",
            "remote: Enumerating objects: 58, done.\u001b[K\n",
            "remote: Counting objects: 100% (58/58), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 58 (delta 22), reused 45 (delta 14), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (58/58), 16.82 KiB | 8.41 MiB/s, done.\n",
            "Resolving deltas: 100% (22/22), done.\n",
            "/content/Text-to-Photo-realistic-Image-Synthesis-with-Stacked-Generative-Adversarial-Networks/code/Text-to-Photo-realistic-Image-Synthesis-with-Stacked-Generative-Adversarial-Networks/code\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import torch, datetime, dateutil.tz\n",
        "import torchvision.transforms as transforms\n",
        "import config as cfg\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import TextDataset\n",
        "from train import GANTrainer\n",
        "import torch.utils.data as data\n",
        "import os.path\n",
        "import PIL\n",
        "import pickle\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "import config as cfg\n",
        "import os\n",
        "import torch\n",
        "import time\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torchfile\n",
        "import numpy as np\n",
        "import pickle\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from utils import mkdir_p\n",
        "from torch.autograd import Variable\n",
        "from torchvision.models.inception import inception_v3\n",
        "from scipy.stats import entropy\n",
        "from torch.nn import functional as F\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import config as cfg\n",
        "from model import Stage1_G, Stage1_D\n",
        "from utils import weights_init, discriminator_loss, generator_loss, KL_loss, save_img_results, save_model"
      ],
      "metadata": {
        "id": "cje9tqEzm3OI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomDataset(Dataset):\n",
        "    def __init__(self, data_size, embedding_size):\n",
        "        self.data_size = data_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.random_images = torch.randn(self.data_size, 3, 64, 64)\n",
        "        self.random_embeddings = torch.randn(self.data_size,\n",
        "                                             self.embedding_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {'real_img_cpu': self.random_images[idx],\n",
        "                'txt_embedding': self.random_embeddings[idx]}\n",
        "\n",
        "random_dataset = RandomDataset(data_size=1000, embedding_size=4800)\n",
        "batch_size = 32\n",
        "data_loader = DataLoader(dataset=random_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "faAF7hz2xwUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GANTrainer(object):\n",
        "    def __init__(self, output_dir):\n",
        "        if cfg.TRAIN_FLAG:\n",
        "            self.model_dir = os.path.join(output_dir, 'Model')\n",
        "            self.image_dir = os.path.join(output_dir, 'Image')\n",
        "            self.log_dir = os.path.join(output_dir, 'Log')\n",
        "            mkdir_p(self.model_dir)\n",
        "            mkdir_p(self.image_dir)\n",
        "            mkdir_p(self.log_dir)\n",
        "\n",
        "        self.gpus = [0]\n",
        "        self.max_epoch = cfg.TRAIN_MAX_EPOCH\n",
        "        self.snapshot_interval = cfg.TRAIN_SNAPSHOT_INTERVAL\n",
        "        self.batch_size = cfg.TRAIN_BATCH_SIZE\n",
        "        torch.cuda.device(0)\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    def get_imgs(self):\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize(32),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "        ])\n",
        "        imgs = []\n",
        "        for i in range(0, 2911):\n",
        "            img = Image.open(\"../data/birds/models/netG_epoch_360/\" + str(i) + \".png\").convert('RGB')\n",
        "            load_size = int(cfg.IMSIZE * 76 / 64)\n",
        "            img = img.resize((load_size, load_size), PIL.Image.BILINEAR)\n",
        "            img = transform(img)\n",
        "            img = np.array(img)\n",
        "            imgs.append(img)\n",
        "        return imgs\n",
        "\n",
        "    def load_network_stageI(self):\n",
        "        netG = Stage1_G()\n",
        "        netG.apply(weights_init)\n",
        "        print(netG)\n",
        "        netD = Stage1_D()\n",
        "        netD.apply(weights_init)\n",
        "        print(netD)\n",
        "\n",
        "        if cfg.NET_G != '':\n",
        "            state_dict = torch.load(cfg.NET_G, map_location=lambda storage, loc: storage)\n",
        "            netG.load_state_dict(state_dict)\n",
        "            print('Load from: ', cfg.NET_G)\n",
        "        if cfg.NET_D != '':\n",
        "            state_dict = torch.load(cfg.NET_D, map_location=lambda storage, loc: storage)\n",
        "            netD.load_state_dict(state_dict)\n",
        "            print('Load from: ', cfg.NET_D)\n",
        "\n",
        "        if cfg.CUDA:\n",
        "            netG.cuda()\n",
        "            netD.cuda()\n",
        "        return netG, netD\n",
        "\n",
        "    def load_network_stageII(self):\n",
        "        from model import Stage1_G, Stage2_G, Stage2_D\n",
        "        Stage1_G = Stage1_G()\n",
        "        netG = Stage2_G(Stage1_G)\n",
        "        netG.apply(weights_init)\n",
        "        print(netG)\n",
        "        if cfg.NET_G != '':\n",
        "            state_dict = torch.load(cfg.NET_G,\n",
        "                                    map_location=lambda storage, loc: storage)\n",
        "            netG.load_state_dict(state_dict)\n",
        "            print('Load from: ', cfg.NET_G)\n",
        "        elif cfg.STAGE1_G != '':\n",
        "            state_dict = torch.load(cfg.STAGE1_G,\n",
        "                                    map_location=lambda storage, loc: storage)\n",
        "            netG.Stage1_G.load_state_dict(state_dict)\n",
        "            print('Load from: ', cfg.STAGE1_G)\n",
        "        else:\n",
        "            print(\"Please give the Stage1_G path\")\n",
        "            return\n",
        "\n",
        "        netD = Stage2_D()\n",
        "        netD.apply(weights_init)\n",
        "        if cfg.NET_D != '':\n",
        "            state_dict = \\\n",
        "                torch.load(cfg.NET_D,\n",
        "                           map_location=lambda storage, loc: storage)\n",
        "            netD.load_state_dict(state_dict)\n",
        "            print('Load from: ', cfg.NET_D)\n",
        "        print(netD)\n",
        "\n",
        "        if cfg.CUDA:\n",
        "            netG.cuda()\n",
        "            netD.cuda()\n",
        "        return netG, netD\n",
        "\n",
        "    def train(self, data_loader, stage=1):\n",
        "        if stage == 1:\n",
        "            netG, netD = self.load_network_stageI()\n",
        "        else:\n",
        "            netG, netD = self.load_network_stageII()\n",
        "\n",
        "        nz = cfg.Z_DIM\n",
        "        batch_size = self.batch_size\n",
        "        noise = Variable(torch.FloatTensor(batch_size, nz))\n",
        "        fixed_noise = Variable(torch.FloatTensor(batch_size, nz).normal_(0, 1), requires_grad=True)\n",
        "        real_labels = Variable(torch.FloatTensor(batch_size).fill_(1))\n",
        "        fake_labels = Variable(torch.FloatTensor(batch_size).fill_(0))\n",
        "\n",
        "        if cfg.CUDA:\n",
        "            noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
        "            real_labels, fake_labels = real_labels.cuda(), fake_labels.cuda()\n",
        "\n",
        "        generator_lr = cfg.TRAIN_GENERATOR_LR\n",
        "        discriminator_lr = cfg.TRAIN_DISCRIMINATOR_LR\n",
        "        lr_decay_step = cfg.TRAIN_LR_DECAY_EPOCH\n",
        "        optimizerD = optim.Adam(netD.parameters(), lr=cfg.TRAIN_DISCRIMINATOR_LR, betas=(0.5, 0.999))\n",
        "\n",
        "        netG_para = []\n",
        "        for p in netG.parameters():\n",
        "            if p.requires_grad:\n",
        "                netG_para.append(p)\n",
        "        optimizerG = optim.Adam(netG_para, lr=cfg.TRAIN_GENERATOR_LR, betas=(0.5, 0.999))\n",
        "\n",
        "        count = 0\n",
        "        for epoch in range(self.max_epoch):\n",
        "            start_t = time.time()\n",
        "            if epoch % lr_decay_step == 0 and epoch > 0:\n",
        "                generator_lr *= 0.5\n",
        "                for param_group in optimizerG.param_groups:\n",
        "                    param_group['lr'] = generator_lr\n",
        "                discriminator_lr *= 0.5\n",
        "                for param_group in optimizerD.param_groups:\n",
        "                    param_group['lr'] = discriminator_lr\n",
        "            for i, data in enumerate(data_loader, 0):\n",
        "                # Prepare training data\n",
        "                real_img_cpu, txt_embedding = data\n",
        "                real_imgs = Variable(real_img_cpu)\n",
        "                txt_embedding = Variable(txt_embedding)\n",
        "                if cfg.CUDA:\n",
        "                    real_imgs = real_imgs.cuda()\n",
        "                    txt_embedding = txt_embedding.cuda()\n",
        "                # Generate fake images\n",
        "                noise.data.normal_(0, 1)\n",
        "                inputs = (txt_embedding, noise)\n",
        "                _, fake_imgs, mu, logvar = nn.parallel.data_parallel(netG, inputs, self.gpus)\n",
        "\n",
        "                # Update D network\n",
        "\n",
        "                netD.zero_grad()\n",
        "                errD, errD_real, errD_wrong, errD_fake = discriminator_loss(netD, real_imgs, fake_imgs,\n",
        "                                                                            real_labels, fake_labels, mu,\n",
        "                                                                            self.gpus)\n",
        "                errD.backward()\n",
        "                optimizerD.step()\n",
        "                ############################\n",
        "                # (2) Update G network\n",
        "                ###########################\n",
        "                netG.zero_grad()\n",
        "                errG = generator_loss(netD, fake_imgs,\n",
        "                                      real_labels, mu, self.gpus)\n",
        "                kl_loss = KL_loss(mu, logvar)\n",
        "                errG_total = errG + kl_loss * cfg.TRAIN_COEFF_KL\n",
        "                errG_total.backward()\n",
        "                optimizerG.step()\n",
        "\n",
        "                count = count + 1\n",
        "\n",
        "                if i % 100 == 0:\n",
        "\n",
        "                    # save the image result for each epoch\n",
        "                    inputs = (txt_embedding, fixed_noise)\n",
        "                    lr_fake, fake, _, _ = \\\n",
        "                        nn.parallel.data_parallel(netG, inputs, self.gpus)\n",
        "                    save_img_results(real_img_cpu, fake, epoch, self.image_dir)\n",
        "                    if lr_fake is not None:\n",
        "                        save_img_results(None, lr_fake, epoch, self.image_dir)\n",
        "                end_t = time.time()\n",
        "                print('''[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f Loss_KL: %.4f\n",
        "                                     Loss_real: %.4f Loss_wrong:%.4f Loss_fake %.4f\n",
        "                                     Total Time: %.2fsec\n",
        "                                  '''\n",
        "                      % (epoch, self.max_epoch, i, len(data_loader),\n",
        "                         errD.data, errG.data, kl_loss.data,\n",
        "                         errD_real, errD_wrong, errD_fake, (end_t - start_t)))\n",
        "                if epoch % self.snapshot_interval == 0:\n",
        "                    save_model(netG, netD, epoch, self.model_dir)\n",
        "                #\n",
        "            save_model(netG, netD, self.max_epoch, self.model_dir)\n",
        "\n",
        "    def inception_score(self, dataloader, cuda=True, batch_size=32, resize=False, splits=1):\n",
        "        \"\"\"Computes the inception score of the generated images imgs\n",
        "        imgs -- Torch dataset of (3xHxW) numpy images normalized in the range [-1, 1]\n",
        "        cuda -- whether or not to run on GPU\n",
        "        batch_size -- batch size for feeding into Inception v3\n",
        "        splits -- number of splits\n",
        "        \"\"\"\n",
        "\n",
        "        imgs = self.get_imgs()\n",
        "        N = 2912\n",
        "        # Set up dtype\n",
        "        if cuda:\n",
        "            dtype = torch.cuda.FloatTensor\n",
        "        else:\n",
        "            if torch.cuda.is_available():\n",
        "                print(\"WARNING: You have a CUDA device, so you should probably set cuda=True\")\n",
        "            dtype = torch.FloatTensor\n",
        "\n",
        "        dataloader = torch.utils.data.DataLoader(imgs, batch_size=batch_size)\n",
        "\n",
        "        # Load inception model\n",
        "        inception_model = inception_v3(pretrained=True, transform_input=False).type(dtype)\n",
        "        inception_model.eval()\n",
        "        up = nn.Upsample(size=(299, 299), mode='bilinear', align_corners=True).type(dtype)\n",
        "\n",
        "        def get_pred(x):\n",
        "            if resize:\n",
        "                x = up(x)\n",
        "            x = inception_model(x)\n",
        "            return F.softmax(x, dim=0).data.cpu().numpy()\n",
        "\n",
        "        # Get predictions\n",
        "        preds = np.zeros((N, 1000))\n",
        "\n",
        "        for i, batch in enumerate(dataloader, 0):\n",
        "            batch = batch.type(dtype)\n",
        "            batchv = Variable(batch)\n",
        "            batch_size_i = batch.size()[0]\n",
        "            preds[i * batch_size:i * batch_size + batch_size_i] = get_pred(batchv)\n",
        "\n",
        "        # Now compute the mean kl-div\n",
        "        split_scores = []\n",
        "\n",
        "        for k in range(splits):\n",
        "            part = preds[k * (N // splits): (k + 1) * (N // splits) - 1, :]\n",
        "            py = np.mean(part, axis=0)\n",
        "            scores = []\n",
        "            for i in range(part.shape[0]):\n",
        "                pyx = part[i, :]\n",
        "                ent = entropy(pyx, py)\n",
        "                scores.append(ent)\n",
        "            split_scores.append(np.exp(np.mean(scores)))\n",
        "\n",
        "        return np.mean(split_scores), np.std(split_scores)\n",
        "\n",
        "    def sample(self, data_loader, stage=1):\n",
        "        if stage == 1:\n",
        "            netG, _ = self.load_network_stageI()\n",
        "        else:\n",
        "            netG, _ = self.load_network_stageII()\n",
        "        netG.eval()\n",
        "\n",
        "        # path to save generated samples\n",
        "        save_dir = cfg.NET_G[:cfg.NET_G.find('.pth')]\n",
        "        if not os.path.isdir(save_dir):\n",
        "            mkdir_p(save_dir)\n",
        "\n",
        "        nz = cfg.Z_DIM\n",
        "        batch_size = self.batch_size\n",
        "        noise = Variable(torch.FloatTensor(batch_size, nz))\n",
        "\n",
        "        count = 0\n",
        "        for i, data in enumerate(data_loader, 0):\n",
        "            real_img_cpu, txt_embedding = data\n",
        "            txt_embedding = Variable(txt_embedding)\n",
        "\n",
        "            if cfg.CUDA:\n",
        "                txt_embedding = txt_embedding.cuda()\n",
        "            noise.data.normal_(0, 1)\n",
        "            inputs = (txt_embedding, noise)\n",
        "            _, fake_imgs, mu, logvar = \\\n",
        "                nn.parallel.data_parallel(netG, inputs, self.gpus)\n",
        "            for i in range(batch_size):\n",
        "                save_name = '%s/%d.png' % (save_dir, count + i)\n",
        "                im = fake_imgs[i].data.cpu().numpy()\n",
        "                im = (im + 1.0) * 127.5\n",
        "                im = im.astype(np.uint8)\n",
        "                im = np.transpose(im, (1, 2, 0))\n",
        "                im = Image.fromarray(im)\n",
        "                im.save(save_name)\n",
        "            count += batch_size"
      ],
      "metadata": {
        "id": "RYpjUGMbzwGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"/content\"\n",
        "algo = GANTrainer(output_dir)\n",
        "algo.train(data_loader, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hGgzX6xkxt1r",
        "outputId": "64773ad6-2565-453b-acd8-eec1b8e96ce9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage1_G(\n",
            "  (ca_net): Ca_Net(\n",
            "    (fc): Linear(in_features=1024, out_features=256, bias=True)\n",
            "    (relu): ReLU()\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=228, out_features=24576, bias=False)\n",
            "    (1): BatchNorm1d(24576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (upsample1): Sequential(\n",
            "    (0): Upsample(scale_factor=2.0, mode='nearest')\n",
            "    (1): Conv2d(1536, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (3): ReLU(inplace=True)\n",
            "  )\n",
            "  (upsample2): Sequential(\n",
            "    (0): Upsample(scale_factor=2.0, mode='nearest')\n",
            "    (1): Conv2d(768, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (3): ReLU(inplace=True)\n",
            "  )\n",
            "  (upsample3): Sequential(\n",
            "    (0): Upsample(scale_factor=2.0, mode='nearest')\n",
            "    (1): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (3): ReLU(inplace=True)\n",
            "  )\n",
            "  (upsample4): Sequential(\n",
            "    (0): Upsample(scale_factor=2.0, mode='nearest')\n",
            "    (1): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (3): ReLU(inplace=True)\n",
            "  )\n",
            "  (img): Sequential(\n",
            "    (0): Conv2d(96, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): Tanh()\n",
            "  )\n",
            ")\n",
            "Stage1_D(\n",
            "  (encode_img): Sequential(\n",
            "    (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (2): Conv2d(96, 192, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (5): Conv2d(192, 384, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (6): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (8): Conv2d(384, 768, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (9): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "  )\n",
            "  (get_cond_logits): D_Logits(\n",
            "    (outlogits): Sequential(\n",
            "      (0): Conv2d(896, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "      (3): Conv2d(768, 1, kernel_size=(4, 4), stride=(4, 4))\n",
            "      (4): Sigmoid()\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-171d642ef151>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0malgo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGANTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0malgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-86269e363f6d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_loader, stage)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mnetG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_network_stageI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mnetG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_network_stageII\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-86269e363f6d>\u001b[0m in \u001b[0;36mload_network_stageI\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCUDA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mnetG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mnetD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnetG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m         \"\"\"\n\u001b[0;32m--> 918\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m         \"\"\"\n\u001b[0;32m--> 918\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ]
    }
  ]
>>>>>>> d1c9b3c3cc183b0e99f0c407ca66a537dd155380
}
