{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y4A9SyiB9uFw",
    "outputId": "77a31eaa-e033-4e1a-d329-b19240ccec9b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchfile\n",
      "  Downloading torchfile-0.1.0.tar.gz (5.2 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting data\n",
      "  Downloading data-0.4.tar.gz (7.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/lib/python3/dist-packages (from data) (1.16.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from data) (5.1.1)\n",
      "Collecting funcsigs (from data)\n",
      "  Downloading funcsigs-1.0.2-py2.py3-none-any.whl (17 kB)\n",
      "Building wheels for collected packages: torchfile, data\n",
      "  Building wheel for torchfile (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for torchfile: filename=torchfile-0.1.0-py3-none-any.whl size=5694 sha256=592e0e190c008a6cbf75f0032b8e8701fa9d78ffca79f55dc81a945c230284f0\n",
      "  Stored in directory: /root/.cache/pip/wheels/c7/e9/87/1c51daf8e468d5c14931f8ac3344880f903ba96b063675cac2\n",
      "  Building wheel for data (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for data: filename=data-0.4-py3-none-any.whl size=7227 sha256=5396eea152c9b6bd7704279a747adc07e6d4dcb90e60c2bc9e5ae07cc5cee825\n",
      "  Stored in directory: /root/.cache/pip/wheels/d0/e8/fa/e253c256048ea58d99a8abb5e751abb6a838af6f12887b5418\n",
      "Successfully built torchfile data\n",
      "Installing collected packages: torchfile, funcsigs, data\n",
      "Successfully installed data-0.4 funcsigs-1.0.2 torchfile-0.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Cloning into 'Text-to-Photo-realistic-Image-Synthesis-with-Stacked-Generative-Adversarial-Networks'...\n",
      "remote: Enumerating objects: 58, done.\u001b[K\n",
      "remote: Counting objects: 100% (58/58), done.\u001b[K\n",
      "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
      "remote: Total 58 (delta 22), reused 45 (delta 14), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (58/58), 16.82 KiB | 8.41 MiB/s, done.\n",
      "Resolving deltas: 100% (22/22), done.\n",
      "/workspace/Text-to-Photo-realistic-Image-Synthesis-with-Stacked-Generative-Adversarial-Networks/code\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "!pip install torchfile data scipy\n",
    "!git clone https://github.com/zeusm9/Text-to-Photo-realistic-Image-Synthesis-with-Stacked-Generative-Adversarial-Networks.git\n",
    "%cd Text-to-Photo-realistic-Image-Synthesis-with-Stacked-Generative-Adversarial-Networks/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "id": "cje9tqEzm3OI"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch, datetime, dateutil.tz\n",
    "import torchvision.transforms as transforms\n",
    "import config as cfg\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import TextDataset\n",
    "import torch.utils.data as data\n",
    "import os.path\n",
    "import PIL\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import config as cfg\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torchfile\n",
    "import numpy as np\n",
    "import pickle\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from utils import mkdir_p\n",
    "from torch.autograd import Variable\n",
    "from torchvision.models.inception import inception_v3\n",
    "from scipy.stats import entropy\n",
    "from torch.nn import functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import config as cfg\n",
    "from utils import weights_init, discriminator_loss, generator_loss, KL_loss, save_img_results, save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stage1_G(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Stage1_G, self).__init__()\n",
    "        self.gf_dim = cfg.GAN_GF_DIM * 8\n",
    "        self.ef_dim = 114\n",
    "        self.z_dim = 114\n",
    "        self.define_module()\n",
    "\n",
    "    def define_module(self):\n",
    "        ninput = self.z_dim + self.ef_dim\n",
    "        ngf = self.gf_dim\n",
    "\n",
    "        self.ca_net = Ca_Net()\n",
    "\n",
    "        # ngf x 4 x 4\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(ninput, ngf * 4 * 4, bias=False),\n",
    "            nn.BatchNorm1d(ngf * 4 * 4),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        # ngf x 4 x 4 -> ngf/2 x 8 x 8\n",
    "        self.upsample1 = upBlock(ngf, ngf // 2)\n",
    "        # -> ngf/4 x 16 x 16\n",
    "        self.upsample2 = upBlock(ngf // 2, ngf // 4)\n",
    "        # -> ngf/8 x 32 x 32\n",
    "        self.upsample3 = upBlock(ngf // 4, ngf // 8)\n",
    "        # -> ngf/16 x 64 x 64\n",
    "        self.upsample4 = upBlock(ngf // 8, ngf // 16)\n",
    "        # -> 3 x 64 x 64\n",
    "        self.img = nn.Sequential(conv3x3(ngf // 16, 3), nn.Tanh())\n",
    "\n",
    "    def forward(self, text_embedding, noise):\n",
    "\n",
    "        print(\"text embeddings shape:\",text_embedding.size() ,\"noise shape:\",noise.size())\n",
    "        c_code, mu, logvar = self.ca_net(text_embedding)\n",
    "        print(\"c_code size:\",c_code.size())\n",
    "        z_c_code = torch.cat((noise, c_code), 1)\n",
    "        print(\"z_c_code size:\",z_c_code.size())\n",
    "        \n",
    "        h_code = self.fc(z_c_code)\n",
    "\n",
    "        h_code = h_code.view(-1, self.gf_dim, 4, 4)\n",
    "        h_code = self.upsample1(h_code)\n",
    "        h_code = self.upsample2(h_code)\n",
    "        h_code = self.upsample3(h_code)\n",
    "        h_code = self.upsample4(h_code)\n",
    "\n",
    "        fake_img = self.img(h_code)\n",
    "        return None, fake_img, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stage1_D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Stage1_D, self).__init__()\n",
    "        self.df_dim = cfg.GAN_DF_DIM\n",
    "        self.ef_dim = cfg.GAN_CONDITION_DIM\n",
    "        self.define_module()\n",
    "\n",
    "    def define_module(self):\n",
    "        ndf, nef = self.df_dim, self.ef_dim\n",
    "        self.encode_img = nn.Sequential(\n",
    "            nn.Conv2d(3, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            # state size (ndf * 8) x 4 x 4)\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        self.get_cond_logits = D_Logits(ndf, nef)\n",
    "        self.get_uncond_logits = None\n",
    "\n",
    "    def forward(self, image):\n",
    "        img_embedding = self.encode_img(image)\n",
    "\n",
    "        return img_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stage2_G(nn.Module):\n",
    "    def __init__(self, Stage1_G):\n",
    "        super(Stage2_G, self).__init__()\n",
    "        self.gf_dim = cfg.GAN_GF_DIM\n",
    "        self.ef_dim = cfg.GAN_CONDITION_DIM\n",
    "        self.z_dim = cfg.Z_DIM\n",
    "        self.Stage1_G = Stage1_G\n",
    "\n",
    "        for param in self.Stage1_G.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.define_module()\n",
    "\n",
    "    def _make_layer(self, block, channel_num):\n",
    "        layers = []\n",
    "        for i in range(cfg.GAN_R_NUM):\n",
    "            layers.append(block(channel_num))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def define_module(self):\n",
    "        ngf = self.gf_dim\n",
    "        self.ca_net = Ca_Net()\n",
    "        self.encoder = nn.Sequential(\n",
    "            conv3x3(3, ngf),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(ngf, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(ngf * 2, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True))\n",
    "        self.hr_joint = nn.Sequential(\n",
    "            conv3x3(self.ef_dim + ngf * 4, ngf * 4),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.residual = self._make_layer(ResBlock, ngf * 4)\n",
    "        self.upsample1 = upBlock(ngf * 4, ngf * 2)\n",
    "        self.upsample2 = upBlock(ngf * 2, ngf)\n",
    "        self.upsample3 = upBlock(ngf, ngf // 2)\n",
    "        self.upsample4 = upBlock(ngf // 2, ngf // 4)\n",
    "        self.img = nn.Sequential(\n",
    "            conv3x3(ngf // 4, 3),\n",
    "            nn.Tanh())\n",
    "\n",
    "    def forward(self, text_embedding, noise):\n",
    "        _, stage1_img, _, _ = self.Stage1_G(text_embedding, noise)\n",
    "        stage1_img = stage1_img.detach()\n",
    "        encoded_img = self.encoder(stage1_img)\n",
    "\n",
    "        c_code, mu, logvar = self.ca_net(text_embedding)\n",
    "        c_code = c_code.view(-1, self.ef_dim, 1, 1)\n",
    "        c_code = c_code.repeat(1, 1, 16, 16)\n",
    "        i_c_code = torch.cat([encoded_img, c_code], 1)\n",
    "        h_code = self.hr_joint(i_c_code)\n",
    "        h_code = self.residual(h_code)\n",
    "\n",
    "        h_code = self.upsample1(h_code)\n",
    "        h_code = self.upsample2(h_code)\n",
    "        h_code = self.upsample3(h_code)\n",
    "        h_code = self.upsample4(h_code)\n",
    "\n",
    "        fake_img = self.img(h_code)\n",
    "        return stage1_img, fake_img, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stage2_D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Stage2_D, self).__init__()\n",
    "        self.df_dim = cfg.GAN_DF_DIM\n",
    "        self.ef_dim = cfg.GAN_CONDITION_DIM\n",
    "        self.define_module()\n",
    "\n",
    "    def define_module(self):\n",
    "        ndf, nef = self.df_dim, self.ef_dim\n",
    "        self.encode_img = nn.Sequential(\n",
    "            nn.Conv2d(3, ndf, 4, 2, 1, bias=False),  # 128 * 128 * ndf\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # 64 * 64 * ndf * 2\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # 32 * 32 * ndf * 4\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # 16 * 16 * ndf * 8\n",
    "            nn.Conv2d(ndf * 8, ndf * 16, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # 8 * 8 * ndf * 16\n",
    "            nn.Conv2d(ndf * 16, ndf * 32, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # 4 * 4 * ndf * 32\n",
    "            conv3x3(ndf * 32, ndf * 16),\n",
    "            nn.BatchNorm2d(ndf * 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # 4 * 4 * ndf * 16\n",
    "            conv3x3(ndf * 16, ndf * 8),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True)  # 4 * 4 * ndf * 8\n",
    "        )\n",
    "\n",
    "        self.get_cond_logits = D_GET_LOGITS(ndf, nef, bcondition=True)\n",
    "        self.get_uncond_logits = D_GET_LOGITS(ndf, nef, bcondition=False)\n",
    "\n",
    "    def forward(self, image):\n",
    "        img_embedding = self.encode_img(image)\n",
    "\n",
    "        return img_embedding\n",
    "\n",
    "\n",
    "class Ca_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Ca_Net, self).__init__()\n",
    "        self.t_dim = cfg.TEXT_DIMENSION\n",
    "        self.c_dim = cfg.GAN_CONDITION_DIM\n",
    "        self.fc = nn.Linear(self.t_dim, self.c_dim * 2, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def encode(self, text_embedding):\n",
    "        x = self.relu(self.fc(text_embedding))\n",
    "        mu = x[:, :self.c_dim]\n",
    "        logvar = x[:, self.c_dim:]\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        if cfg.CUDA:\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def forward(self, text_embedding):\n",
    "        mu, logvar = self.encode(text_embedding)\n",
    "        c_code = self.reparametrize(mu, logvar)\n",
    "        return c_code, mu, logvar\n",
    "\n",
    "\n",
    "class D_Logits(nn.Module):\n",
    "    def __init__(self, ndf, nef, bcondition=True):\n",
    "        super(D_Logits, self).__init__()\n",
    "        self.df_dim = ndf\n",
    "        self.ef_dim = nef\n",
    "        self.bcondition = bcondition\n",
    "\n",
    "        if bcondition:\n",
    "            self.outlogits = nn.Sequential(\n",
    "                conv3x3(ndf * 8 + nef, ndf * 8),\n",
    "                nn.BatchNorm2d(ndf * 8),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Conv2d(ndf * 8, 1, kernel_size=4, stride=4),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        else:\n",
    "            self.outlogits = nn.Sequential(\n",
    "                nn.Conv2d(ndf * 8, 1, kernel_size=4, stride=4),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "    def forward(self, h_code, c_code=None):\n",
    "        if self.bcondition and c_code is not None:\n",
    "            c_code = c_code.view(-1, self.ef_dim, 1, 1)\n",
    "            c_code = c_code.repeat(1, 1, 4, 4)\n",
    "            h_c_code = torch.cat((h_code, c_code), 1)\n",
    "        else:\n",
    "            h_c_code = h_code\n",
    "        output = self.outlogits(h_c_code)\n",
    "        return output.view(-1)\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channel_num):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            conv3x3(channel_num, channel_num),\n",
    "            nn.BatchNorm2d(channel_num),\n",
    "            nn.ReLU(True),\n",
    "            conv3x3(channel_num, channel_num),\n",
    "            nn.BatchNorm2d(channel_num))\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.block(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class D_GET_LOGITS(nn.Module):\n",
    "    def __init__(self, ndf, nef, bcondition=True):\n",
    "        super(D_GET_LOGITS, self).__init__()\n",
    "        self.df_dim = ndf\n",
    "        self.ef_dim = nef\n",
    "        self.bcondition = bcondition\n",
    "        if bcondition:\n",
    "            self.outlogits = nn.Sequential(\n",
    "                conv3x3(ndf * 8 + nef, ndf * 8),\n",
    "                nn.BatchNorm2d(ndf * 8),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Conv2d(ndf * 8, 1, kernel_size=4, stride=4),\n",
    "                nn.Sigmoid())\n",
    "        else:\n",
    "            self.outlogits = nn.Sequential(\n",
    "                nn.Conv2d(ndf * 8, 1, kernel_size=4, stride=4),\n",
    "                nn.Sigmoid())\n",
    "\n",
    "    def forward(self, h_code, c_code=None):\n",
    "        # conditioning output\n",
    "        if self.bcondition and c_code is not None:\n",
    "            c_code = c_code.view(-1, self.ef_dim, 1, 1)\n",
    "            c_code = c_code.repeat(1, 1, 4, 4)\n",
    "            # state size (ngf+egf) x 4 x 4\n",
    "            h_c_code = torch.cat((h_code, c_code), 1)\n",
    "        else:\n",
    "            h_c_code = h_code\n",
    "\n",
    "        output = self.outlogits(h_c_code)\n",
    "        return output.view(-1)\n",
    "\n",
    "\n",
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "def upBlock(in_channels, out_channels):\n",
    "    block = nn.Sequential(\n",
    "        nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "        conv3x3(in_channels, out_channels),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(True)\n",
    "    )\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "id": "RYpjUGMbzwGZ"
   },
   "outputs": [],
   "source": [
    "class GANTrainer(object):\n",
    "    def __init__(self, output_dir):\n",
    "        if cfg.TRAIN_FLAG:\n",
    "            self.model_dir = os.path.join(output_dir, 'Model')\n",
    "            self.image_dir = os.path.join(output_dir, 'Image')\n",
    "            self.log_dir = os.path.join(output_dir, 'Log')\n",
    "\n",
    "        self.gpus = [0]\n",
    "        self.max_epoch = cfg.TRAIN_MAX_EPOCH\n",
    "        self.snapshot_interval = cfg.TRAIN_SNAPSHOT_INTERVAL\n",
    "        self.batch_size = 8\n",
    "        torch.cuda.device(0)\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "    def get_imgs(self):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(32),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        ])\n",
    "        imgs = []\n",
    "        for i in range(0, 2911):\n",
    "            img = Image.open(\"../data/birds/models/netG_epoch_360/\" + str(i) + \".png\").convert('RGB')\n",
    "            load_size = int(cfg.IMSIZE * 76 / 64)\n",
    "            img = img.resize((load_size, load_size), PIL.Image.BILINEAR)\n",
    "            img = transform(img)\n",
    "            img = np.array(img)\n",
    "            imgs.append(img)\n",
    "        return imgs\n",
    "\n",
    "    def load_network_stageI(self):\n",
    "        netG = Stage1_G()\n",
    "        netG.apply(weights_init)\n",
    "        print(netG)\n",
    "        netD = Stage1_D()\n",
    "        netD.apply(weights_init)\n",
    "        print(netD)\n",
    "\n",
    "        if cfg.NET_G != '':\n",
    "            state_dict = torch.load(cfg.NET_G, map_location=lambda storage, loc: storage)\n",
    "            netG.load_state_dict(state_dict)\n",
    "            print('Load from: ', cfg.NET_G)\n",
    "        if cfg.NET_D != '':\n",
    "            state_dict = torch.load(cfg.NET_D, map_location=lambda storage, loc: storage)\n",
    "            netD.load_state_dict(state_dict)\n",
    "            print('Load from: ', cfg.NET_D)\n",
    "\n",
    "        if cfg.CUDA:\n",
    "            netG.cuda()\n",
    "            netD.cuda()\n",
    "        return netG, netD\n",
    "\n",
    "    def load_network_stageII(self):\n",
    "        from model import Stage1_G, Stage2_G, Stage2_D\n",
    "        Stage1_G = Stage1_G()\n",
    "        netG = Stage2_G(Stage1_G)\n",
    "        netG.apply(weights_init)\n",
    "        print(netG)\n",
    "        if cfg.NET_G != '':\n",
    "            state_dict = torch.load(cfg.NET_G,\n",
    "                                    map_location=lambda storage, loc: storage)\n",
    "            netG.load_state_dict(state_dict)\n",
    "            print('Load from: ', cfg.NET_G)\n",
    "        elif cfg.STAGE1_G != '':\n",
    "            state_dict = torch.load(cfg.STAGE1_G,\n",
    "                                    map_location=lambda storage, loc: storage)\n",
    "            netG.Stage1_G.load_state_dict(state_dict)\n",
    "            print('Load from: ', cfg.STAGE1_G)\n",
    "        else:\n",
    "            print(\"Please give the Stage1_G path\")\n",
    "            return\n",
    "\n",
    "        netD = Stage2_D()\n",
    "        netD.apply(weights_init)\n",
    "        if cfg.NET_D != '':\n",
    "            state_dict = \\\n",
    "                torch.load(cfg.NET_D,\n",
    "                           map_location=lambda storage, loc: storage)\n",
    "            netD.load_state_dict(state_dict)\n",
    "            print('Load from: ', cfg.NET_D)\n",
    "        print(netD)\n",
    "\n",
    "        if cfg.CUDA:\n",
    "            netG.cuda()\n",
    "            netD.cuda()\n",
    "        return netG, netD\n",
    "\n",
    "    def train(self, data_loader, stage=1):\n",
    "        if stage == 1:\n",
    "            netG, netD = self.load_network_stageI()\n",
    "        else:\n",
    "            netG, netD = self.load_network_stageII()\n",
    "\n",
    "        nz = cfg.Z_DIM\n",
    "        batch_size = self.batch_size\n",
    "        noise = Variable(torch.FloatTensor(batch_size, nz))\n",
    "        fixed_noise = Variable(torch.FloatTensor(batch_size, nz).normal_(0, 1), requires_grad=True)\n",
    "        real_labels = Variable(torch.FloatTensor(batch_size).fill_(1))\n",
    "        fake_labels = Variable(torch.FloatTensor(batch_size).fill_(0))\n",
    "\n",
    "        if cfg.CUDA:\n",
    "            noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
    "            real_labels, fake_labels = real_labels.cuda(), fake_labels.cuda()\n",
    "\n",
    "        generator_lr = cfg.TRAIN_GENERATOR_LR\n",
    "        discriminator_lr = cfg.TRAIN_DISCRIMINATOR_LR\n",
    "        lr_decay_step = cfg.TRAIN_LR_DECAY_EPOCH\n",
    "        optimizerD = optim.Adam(netD.parameters(), lr=cfg.TRAIN_DISCRIMINATOR_LR, betas=(0.5, 0.999))\n",
    "\n",
    "        netG_para = []\n",
    "        for p in netG.parameters():\n",
    "            if p.requires_grad:\n",
    "                netG_para.append(p)\n",
    "        optimizerG = optim.Adam(netG_para, lr=cfg.TRAIN_GENERATOR_LR, betas=(0.5, 0.999))\n",
    "\n",
    "        count = 0\n",
    "        for epoch in range(self.max_epoch):\n",
    "            start_t = time.time()\n",
    "            if epoch % lr_decay_step == 0 and epoch > 0:\n",
    "                generator_lr *= 0.5\n",
    "                for param_group in optimizerG.param_groups:\n",
    "                    param_group['lr'] = generator_lr\n",
    "                discriminator_lr *= 0.5\n",
    "                for param_group in optimizerD.param_groups:\n",
    "                    param_group['lr'] = discriminator_lr\n",
    "            for i, data in enumerate(data_loader, 0):\n",
    "                # Prepare training data\n",
    "                real_img_cpu, txt_embedding = data\n",
    "                real_imgs = Variable(real_img_cpu)\n",
    "                txt_embedding = Variable(txt_embedding)\n",
    "                if cfg.CUDA:\n",
    "                    real_imgs = real_imgs.cuda()\n",
    "                    txt_embedding = txt_embedding.cuda()\n",
    "                # Generate fake images\n",
    "                noise.data.normal_(0, 1)\n",
    "                inputs = (txt_embedding, noise)\n",
    "                _, fake_imgs, mu, logvar = nn.parallel.data_parallel(netG, inputs, self.gpus)\n",
    "\n",
    "                netD.zero_grad()\n",
    "                errD, errD_real, errD_wrong, errD_fake = discriminator_loss(netD, real_imgs, fake_imgs,\n",
    "                                                                            real_labels, fake_labels, mu,\n",
    "                                                                            self.gpus)\n",
    "                errD.backward()\n",
    "                optimizerD.step()\n",
    "\n",
    "                netG.zero_grad()\n",
    "                errG = generator_loss(netD, fake_imgs,\n",
    "                                      real_labels, mu, self.gpus)\n",
    "                kl_loss = KL_loss(mu, logvar)\n",
    "                errG_total = errG + kl_loss * cfg.TRAIN_COEFF_KL\n",
    "                errG_total.backward()\n",
    "                optimizerG.step()\n",
    "\n",
    "                count = count + 1\n",
    "\n",
    "                if i % 100 == 0:\n",
    "\n",
    "                    # save the image result for each epoch\n",
    "                    inputs = (txt_embedding, fixed_noise)\n",
    "                    lr_fake, fake, _, _ = \\\n",
    "                        nn.parallel.data_parallel(netG, inputs, self.gpus)\n",
    "                    save_img_results(real_img_cpu, fake, epoch, self.image_dir)\n",
    "                    if lr_fake is not None:\n",
    "                        save_img_results(None, lr_fake, epoch, self.image_dir)\n",
    "                end_t = time.time()\n",
    "                print('''[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f Loss_KL: %.4f\n",
    "                                     Loss_real: %.4f Loss_wrong:%.4f Loss_fake %.4f\n",
    "                                     Total Time: %.2fsec\n",
    "                                  '''\n",
    "                      % (epoch, self.max_epoch, i, len(data_loader),\n",
    "                         errD.data, errG.data, kl_loss.data,\n",
    "                         errD_real, errD_wrong, errD_fake, (end_t - start_t)))\n",
    "                if epoch % self.snapshot_interval == 0:\n",
    "                    save_model(netG, netD, epoch, self.model_dir)\n",
    "                #\n",
    "            save_model(netG, netD, self.max_epoch, self.model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "id": "faAF7hz2xwUJ"
   },
   "outputs": [],
   "source": [
    "class RandomDataset(Dataset):\n",
    "    def __init__(self, data_size, embedding_size):\n",
    "        self.data_size = data_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.random_images = torch.randn(self.data_size, 3, 64, 64)\n",
    "        self.random_embeddings = torch.randn(self.data_size,\n",
    "                                             self.embedding_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.random_images[idx],self.random_embeddings[idx])\n",
    "\n",
    "random_dataset = RandomDataset(data_size=1000, embedding_size=1024)\n",
    "batch_size = 16\n",
    "data_loader = DataLoader(dataset=random_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hGgzX6xkxt1r",
    "outputId": "64773ad6-2565-453b-acd8-eec1b8e96ce9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage1_G(\n",
      "  (ca_net): Ca_Net(\n",
      "    (fc): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=228, out_features=24576, bias=False)\n",
      "    (1): BatchNorm1d(24576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (upsample1): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='nearest')\n",
      "    (1): Conv2d(1536, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "  )\n",
      "  (upsample2): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='nearest')\n",
      "    (1): Conv2d(768, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "  )\n",
      "  (upsample3): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='nearest')\n",
      "    (1): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "  )\n",
      "  (upsample4): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode='nearest')\n",
      "    (1): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "  )\n",
      "  (img): Sequential(\n",
      "    (0): Conv2d(96, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): Tanh()\n",
      "  )\n",
      ")\n",
      "Stage1_D(\n",
      "  (encode_img): Sequential(\n",
      "    (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(96, 192, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(192, 384, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(384, 768, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (get_cond_logits): D_Logits(\n",
      "    (outlogits): Sequential(\n",
      "      (0): Conv2d(896, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      (3): Conv2d(768, 1, kernel_size=(4, 4), stride=(4, 4))\n",
      "      (4): Sigmoid()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "text embeddings shape: torch.Size([16, 1024]) noise shape: torch.Size([8, 100])\n",
      "c_code size: torch.Size([16, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 8 but got size 16 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[188], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/workspace\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m algo \u001b[38;5;241m=\u001b[39m GANTrainer(output_dir)\n\u001b[0;32m----> 3\u001b[0m \u001b[43malgo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[186], line 137\u001b[0m, in \u001b[0;36mGANTrainer.train\u001b[0;34m(self, data_loader, stage)\u001b[0m\n\u001b[1;32m    135\u001b[0m noise\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnormal_(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    136\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (txt_embedding, noise)\n\u001b[0;32m--> 137\u001b[0m _, fake_imgs, mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m netD\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    140\u001b[0m errD, errD_real, errD_wrong, errD_fake \u001b[38;5;241m=\u001b[39m discriminator_loss(netD, real_imgs, fake_imgs,\n\u001b[1;32m    141\u001b[0m                                                             real_labels, fake_labels, mu,\n\u001b[1;32m    142\u001b[0m                                                             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpus)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/data_parallel.py:265\u001b[0m, in \u001b[0;36mdata_parallel\u001b[0;34m(module, inputs, device_ids, output_device, dim, module_kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m module_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(device_ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m used_device_ids \u001b[38;5;241m=\u001b[39m device_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)]\n\u001b[1;32m    267\u001b[0m replicas \u001b[38;5;241m=\u001b[39m replicate(module, used_device_ids)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[182], line 38\u001b[0m, in \u001b[0;36mStage1_G.forward\u001b[0;34m(self, text_embedding, noise)\u001b[0m\n\u001b[1;32m     36\u001b[0m c_code, mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_net(text_embedding)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc_code size:\u001b[39m\u001b[38;5;124m\"\u001b[39m,c_code\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m---> 38\u001b[0m z_c_code \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_code\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mz_c_code size:\u001b[39m\u001b[38;5;124m\"\u001b[39m,z_c_code\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     41\u001b[0m h_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(z_c_code)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 8 but got size 16 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "output_dir = \"/workspace\"\n",
    "algo = GANTrainer(output_dir)\n",
    "algo.train(data_loader, 1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
