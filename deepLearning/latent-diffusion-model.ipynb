{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0q2zcciDSIZ",
        "outputId": "f104d33b-9c2f-499c-f41f-492ac6c7cf45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'latent-diffusion' already exists and is not an empty directory.\n",
            "/content/latent-diffusion\n",
            "Requirement already satisfied: albumentations==0.4.3 in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: pudb==2019.2 in /usr/local/lib/python3.10/dist-packages (2019.2)\n",
            "Requirement already satisfied: imageio==2.9.0 in /usr/local/lib/python3.10/dist-packages (2.9.0)\n",
            "Requirement already satisfied: imageio-ffmpeg==0.4.2 in /usr/local/lib/python3.10/dist-packages (0.4.2)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from albumentations==0.4.3) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from albumentations==0.4.3) (1.11.4)\n",
            "Requirement already satisfied: imgaug<0.2.7,>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from albumentations==0.4.3) (0.2.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations==0.4.3) (6.0.1)\n",
            "Requirement already satisfied: urwid>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from pudb==2019.2) (2.6.8)\n",
            "Requirement already satisfied: pygments>=1.0 in /usr/local/lib/python3.10/dist-packages (from pudb==2019.2) (2.16.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from imageio==2.9.0) (9.4.0)\n",
            "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.4.3) (0.19.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.4.3) (1.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from urwid>=1.1.1->pudb==2019.2) (4.10.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from urwid>=1.1.1->pudb==2019.2) (0.2.13)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.3) (3.2.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.3) (2024.2.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.3) (1.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.3) (23.2)\n",
            "Requirement already satisfied: pytorch-lightning==1.5.10 in /usr/local/lib/python3.10/dist-packages (1.5.10)\n",
            "Requirement already satisfied: torch-fidelity==0.3.0 in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.5.10) (1.25.2)\n",
            "Requirement already satisfied: torch>=1.7.* in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.5.10) (2.1.0+cu121)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.5.10) (0.18.3)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.5.10) (4.66.2)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.5.10) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.5.10) (2023.6.0)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.5.10) (2.15.2)\n",
            "Requirement already satisfied: torchmetrics>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.5.10) (1.3.1)\n",
            "Requirement already satisfied: pyDeprecate==0.3.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.5.10) (0.3.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.5.10) (23.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.5.10) (4.10.0)\n",
            "Requirement already satisfied: setuptools==59.5.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.5.10) (59.5.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from torch-fidelity==0.3.0) (9.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-fidelity==0.3.0) (1.11.4)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from torch-fidelity==0.3.0) (0.16.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (3.9.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10) (1.62.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10) (3.5.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10) (3.20.3)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10) (3.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.*->pytorch-lightning==1.5.10) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.*->pytorch-lightning==1.5.10) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.*->pytorch-lightning==1.5.10) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.*->pytorch-lightning==1.5.10) (2.1.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics>=0.4.1->pytorch-lightning==1.5.10) (0.10.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (4.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.5.10) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.5.10) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.5.10) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.2.0->pytorch-lightning==1.5.10) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.2.0->pytorch-lightning==1.5.10) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.*->pytorch-lightning==1.5.10) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.5.10) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.2.0->pytorch-lightning==1.5.10) (3.2.2)\n",
            "Obtaining taming-transformers from git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers\n",
            "  Updating ./src/taming-transformers clone (to revision master)\n",
            "  Running command git fetch -q --tags\n",
            "  Running command git reset --hard -q 3ba01b241669f5ade541ce990f7650a3b8f65318\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from taming-transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from taming-transformers) (1.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from taming-transformers) (4.66.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->taming-transformers) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->taming-transformers) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->taming-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->taming-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->taming-transformers) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->taming-transformers) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->taming-transformers) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->taming-transformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->taming-transformers) (1.3.0)\n",
            "Installing collected packages: taming-transformers\n",
            "  Running setup.py develop for taming-transformers\n",
            "Successfully installed taming-transformers-0.0.1\n",
            "Obtaining clip from git+https://github.com/openai/CLIP.git@main#egg=clip\n",
            "  Updating ./src/clip clone (to revision main)\n",
            "  Running command git fetch -q --tags\n",
            "  Running command git reset --hard -q a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from clip)\n",
            "  Using cached ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip) (4.66.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip) (0.16.0+cu121)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip) (1.3.0)\n",
            "Installing collected packages: ftfy, clip\n",
            "  Running setup.py develop for clip\n",
            "Successfully installed clip-1.0 ftfy-6.1.3\n",
            "Obtaining file:///content/latent-diffusion\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from latent-diffusion==0.0.1) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from latent-diffusion==0.0.1) (1.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from latent-diffusion==0.0.1) (4.66.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->latent-diffusion==0.0.1) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->latent-diffusion==0.0.1) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->latent-diffusion==0.0.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->latent-diffusion==0.0.1) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->latent-diffusion==0.0.1) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->latent-diffusion==0.0.1) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->latent-diffusion==0.0.1) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->latent-diffusion==0.0.1) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->latent-diffusion==0.0.1) (1.3.0)\n",
            "Installing collected packages: latent-diffusion\n",
            "  Running setup.py develop for latent-diffusion\n",
            "Successfully installed latent-diffusion-0.0.1\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/CompVis/latent-diffusion.git\n",
        "%cd latent-diffusion\n",
        "!pip install albumentations==0.4.3 opencv-python pudb==2019.2 imageio==2.9.0 imageio-ffmpeg==0.4.2\n",
        "!pip install omegaconf==2.1.1 test-tube>=0.7.5 streamlit>=0.73.1 einops==0.3.0\n",
        "!pip install pytorch-lightning==1.5.10 torch-fidelity==0.3.0 transformers\n",
        "!pip install -e git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers\n",
        "!pip install -e git+https://github.com/openai/CLIP.git@main#egg=clip\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O models/first_stage_models/vq-f4/model.zip https://ommer-lab.com/files/latent-diffusion/vq-f4.zip\n",
        "%cd models/first_stage_models/vq-f4\n",
        "!unzip -o model.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBA0r54xDXC3",
        "outputId": "0a0e9add-dcbe-4639-ffe1-9a3b72b2542c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-05 00:51:14--  https://ommer-lab.com/files/latent-diffusion/vq-f4.zip\n",
            "Resolving ommer-lab.com (ommer-lab.com)... 141.84.41.65\n",
            "Connecting to ommer-lab.com (ommer-lab.com)|141.84.41.65|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 696655056 (664M) [application/zip]\n",
            "Saving to: ‘models/first_stage_models/vq-f4/model.zip’\n",
            "\n",
            "models/first_stage_ 100%[===================>] 664.38M  29.4MB/s    in 23s     \n",
            "\n",
            "2024-03-05 00:51:38 (28.4 MB/s) - ‘models/first_stage_models/vq-f4/model.zip’ saved [696655056/696655056]\n",
            "\n",
            "/content/latent-diffusion/models/first_stage_models/vq-f4\n",
            "Archive:  model.zip\n",
            "  inflating: model.ckpt              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O models/ldm/celeba256/celeba-256.zip https://ommer-lab.com/files/latent-diffusion/celeba.zip\n",
        "%cd models/ldm/celeba256\n",
        "!unzip -o celeba-256.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFBiCBooD0FI",
        "outputId": "a9ec2a8d-64ac-4072-dd4f-ef4e6ca79dc7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-05 00:52:25--  https://ommer-lab.com/files/latent-diffusion/celeba.zip\n",
            "Resolving ommer-lab.com (ommer-lab.com)... 141.84.41.65\n",
            "Connecting to ommer-lab.com (ommer-lab.com)|141.84.41.65|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2239361584 (2.1G) [application/zip]\n",
            "Saving to: ‘models/ldm/celeba256/celeba-256.zip’\n",
            "\n",
            "models/ldm/celeba25 100%[===================>]   2.08G  29.9MB/s    in 74s     \n",
            "\n",
            "2024-03-05 00:53:39 (29.0 MB/s) - ‘models/ldm/celeba256/celeba-256.zip’ saved [2239361584/2239361584]\n",
            "\n",
            "/content/latent-diffusion/models/ldm/celeba256\n",
            "Archive:  celeba-256.zip\n",
            "  inflating: model.ckpt              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ..\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dz2wPxsD351",
        "outputId": "aeed06b9-93aa-4825-d533-1ad88e754e85"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/latent-diffusion\n",
            "'=0.73.1'   configs\t       latent_diffusion.egg-info   main.py\t\t README.md   src\n",
            "'=0.7.5'    data\t       ldm\t\t\t   models\t\t scripts\n",
            " assets     environment.yaml   LICENSE\t\t\t   notebook_helpers.py\t setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/spaces/EleutherAI/VQGAN_CLIP/raw/main/taming-transformers/data/celebahqtrain.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpEzC0R6EyAP",
        "outputId": "ae0c846e-17d8-4f61-96e2-ae6cef16ad8a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-05 00:54:26--  https://huggingface.co/spaces/EleutherAI/VQGAN_CLIP/raw/main/taming-transformers/data/celebahqtrain.txt\n",
            "Resolving huggingface.co (huggingface.co)... 18.238.49.70, 18.238.49.117, 18.238.49.112, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.238.49.70|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 375000 (366K) [text/plain]\n",
            "Saving to: ‘celebahqtrain.txt’\n",
            "\n",
            "\rcelebahqtrain.txt     0%[                    ]       0  --.-KB/s               \rcelebahqtrain.txt   100%[===================>] 366.21K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-03-05 00:54:26 (13.5 MB/s) - ‘celebahqtrain.txt’ saved [375000/375000]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WU4WK4r-iO3C",
        "outputId": "d29ccc98-8ca9-486d-bce5-d87ed829a329"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting diffusers\n",
            "  Downloading diffusers-0.26.3-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers) (7.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.2 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.20.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers) (1.25.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.4.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers) (9.4.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.2->diffusers) (2023.6.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.2->diffusers) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.2->diffusers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.2->diffusers) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.2->diffusers) (23.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2024.2.2)\n",
            "Installing collected packages: diffusers\n",
            "Successfully installed diffusers-0.26.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import AutoencoderKL\n",
        "url = \"https://huggingface.co/stabilityai/sd-vae-ft-mse-original/blob/main/vae-ft-mse-840000-ema-pruned.safetensors\"\n",
        "ema_vae = AutoencoderKL.from_single_file(url)"
      ],
      "metadata": {
        "id": "4ZtWnIViiN_I"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "methods = [method for method in dir(ema_vae) if callable(getattr(ema_vae, method))]"
      ],
      "metadata": {
        "id": "pMrIuAn0kH3a"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "dummy_image = torch.rand(1 ,3, 256, 256)"
      ],
      "metadata": {
        "id": "aFmkLPWdk2ss"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ema_vae.encoder(dummy_image).size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMhXepPSkfVp",
        "outputId": "f14c3735-6e23-4635-8596-d4f989d67a36"
      },
      "execution_count": 18,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 8, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2Config, Wav2Vec2Model\n",
        "configuration = Wav2Vec2Config()\n",
        "Wav2vec = Wav2Vec2Model(configuration)"
      ],
      "metadata": {
        "id": "tu-WheOeiTeJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from einops import rearrange, repeat\n",
        "from contextlib import contextmanager\n",
        "from functools import partial\n",
        "from tqdm import tqdm\n",
        "from torchvision.utils import make_grid\n",
        "from pytorch_lightning.utilities.distributed import rank_zero_only\n",
        "\n",
        "from ldm.util import log_txt_as_img, exists, default, ismap, isimage, mean_flat, count_params, instantiate_from_config\n",
        "from ldm.modules.ema import LitEma\n",
        "from ldm.modules.distributions.distributions import normal_kl, DiagonalGaussianDistribution\n",
        "from ldm.models.autoencoder import VQModelInterface, IdentityFirstStage, AutoencoderKL\n",
        "from ldm.modules.diffusionmodules.util import make_beta_schedule, extract_into_tensor, noise_like\n",
        "from ldm.models.diffusion.ddim import DDIMSampler"
      ],
      "metadata": {
        "id": "mn6TjzU8G1Pd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "__conditioning_keys__ = {'concat': 'c_concat',\n",
        "                         'crossattn': 'c_crossattn',\n",
        "                         'adm': 'y'}\n",
        "\n",
        "def disabled_train(self, mode=True):\n",
        "    \"\"\"Overwrite model.train with this function to make sure train/eval mode\n",
        "    does not change anymore.\"\"\"\n",
        "    return self\n",
        "\n",
        "def uniform_on_device(r1, r2, shape, device):\n",
        "    return (r1 - r2) * torch.rand(*shape, device=device) + r2"
      ],
      "metadata": {
        "id": "if52PA7AG5dO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DiffusionWrapper(pl.LightningModule):\n",
        "    def __init__(self, diff_model_config, conditioning_key):\n",
        "        super().__init__()\n",
        "        self.diffusion_model = instantiate_from_config(diff_model_config)\n",
        "        self.conditioning_key = conditioning_key\n",
        "        assert self.conditioning_key in [None, 'concat', 'crossattn', 'hybrid', 'adm']\n",
        "\n",
        "    def forward(self, x, t, c_concat: list = None, c_crossattn: list = None):\n",
        "        if self.conditioning_key is None:\n",
        "            out = self.diffusion_model(x, t)\n",
        "        elif self.conditioning_key == 'concat':\n",
        "            xc = torch.cat([x] + c_concat, dim=1)\n",
        "            out = self.diffusion_model(xc, t)\n",
        "        elif self.conditioning_key == 'crossattn':\n",
        "            cc = torch.cat(c_crossattn, 1)\n",
        "            out = self.diffusion_model(x, t, context=cc)\n",
        "        elif self.conditioning_key == 'hybrid':\n",
        "            xc = torch.cat([x] + c_concat, dim=1)\n",
        "            cc = torch.cat(c_crossattn, 1)\n",
        "            out = self.diffusion_model(xc, t, context=cc)\n",
        "        elif self.conditioning_key == 'adm':\n",
        "            cc = c_crossattn[0]\n",
        "            out = self.diffusion_model(x, t, y=cc)\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "nMBz6diAIWrR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DDPM(pl.LightningModule):\n",
        "\n",
        "    def __init__(self,\n",
        "                 unet_config,\n",
        "                 timesteps=1000,\n",
        "                 beta_schedule=\"linear\",\n",
        "                 loss_type=\"l2\",\n",
        "                 ckpt_path=None,\n",
        "                 ignore_keys=[],\n",
        "                 load_only_unet=False,\n",
        "                 monitor=\"val/loss\",\n",
        "                 use_ema=True,\n",
        "                 first_stage_key=\"image\",\n",
        "                 image_size=256,\n",
        "                 channels=3,\n",
        "                 log_every_t=100,\n",
        "                 clip_denoised=True,\n",
        "                 linear_start=1e-4,\n",
        "                 linear_end=2e-2,\n",
        "                 cosine_s=8e-3,\n",
        "                 given_betas=None,\n",
        "                 original_elbo_weight=0.,\n",
        "                 v_posterior=0.,\n",
        "                 l_simple_weight=1.,\n",
        "                 conditioning_key=None,\n",
        "                 parameterization=\"eps\",\n",
        "                 scheduler_config=None,\n",
        "                 use_positional_encodings=False,\n",
        "                 learn_logvar=False,\n",
        "                 logvar_init=0.,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        assert parameterization in [\"eps\", \"x0\"], 'currently only supporting \"eps\" and \"x0\"'\n",
        "        self.parameterization = parameterization\n",
        "        print(f\"{self.__class__.__name__}: Running in {self.parameterization}-prediction mode\")\n",
        "        self.cond_stage_model = None\n",
        "        self.clip_denoised = clip_denoised\n",
        "        self.log_every_t = log_every_t\n",
        "        self.first_stage_key = first_stage_key\n",
        "        self.image_size = image_size  # try conv?\n",
        "        self.channels = channels\n",
        "        self.use_positional_encodings = use_positional_encodings\n",
        "        self.model = DiffusionWrapper(unet_config, conditioning_key)\n",
        "        count_params(self.model, verbose=True)\n",
        "        self.use_ema = use_ema\n",
        "        if self.use_ema:\n",
        "            self.model_ema = LitEma(self.model)\n",
        "            print(f\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\")\n",
        "\n",
        "        self.use_scheduler = scheduler_config is not None\n",
        "        if self.use_scheduler:\n",
        "            self.scheduler_config = scheduler_config\n",
        "\n",
        "        self.v_posterior = v_posterior\n",
        "        self.original_elbo_weight = original_elbo_weight\n",
        "        self.l_simple_weight = l_simple_weight\n",
        "\n",
        "        if monitor is not None:\n",
        "            self.monitor = monitor\n",
        "        if ckpt_path is not None:\n",
        "            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys, only_model=load_only_unet)\n",
        "\n",
        "        self.register_schedule(given_betas=given_betas, beta_schedule=beta_schedule, timesteps=timesteps,\n",
        "                               linear_start=linear_start, linear_end=linear_end, cosine_s=cosine_s)\n",
        "\n",
        "        self.loss_type = loss_type\n",
        "\n",
        "        self.learn_logvar = learn_logvar\n",
        "        self.logvar = torch.full(fill_value=logvar_init, size=(self.num_timesteps,))\n",
        "        if self.learn_logvar:\n",
        "            self.logvar = nn.Parameter(self.logvar, requires_grad=True)\n",
        "\n",
        "\n",
        "    def register_schedule(self, given_betas=None, beta_schedule=\"linear\", timesteps=1000,\n",
        "                          linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n",
        "        if exists(given_betas):\n",
        "            betas = given_betas\n",
        "        else:\n",
        "            betas = make_beta_schedule(beta_schedule, timesteps, linear_start=linear_start, linear_end=linear_end,\n",
        "                                       cosine_s=cosine_s)\n",
        "        alphas = 1. - betas\n",
        "        alphas_cumprod = np.cumprod(alphas, axis=0)\n",
        "        alphas_cumprod_prev = np.append(1., alphas_cumprod[:-1])\n",
        "\n",
        "        timesteps, = betas.shape\n",
        "        self.num_timesteps = int(timesteps)\n",
        "        self.linear_start = linear_start\n",
        "        self.linear_end = linear_end\n",
        "        assert alphas_cumprod.shape[0] == self.num_timesteps, 'alphas have to be defined for each timestep'\n",
        "\n",
        "        to_torch = partial(torch.tensor, dtype=torch.float32)\n",
        "\n",
        "        self.register_buffer('betas', to_torch(betas))\n",
        "        self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod))\n",
        "        self.register_buffer('alphas_cumprod_prev', to_torch(alphas_cumprod_prev))\n",
        "\n",
        "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
        "        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod)))\n",
        "        self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod)))\n",
        "        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod)))\n",
        "        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod)))\n",
        "        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod - 1)))\n",
        "\n",
        "        # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
        "        posterior_variance = (1 - self.v_posterior) * betas * (1. - alphas_cumprod_prev) / (\n",
        "                    1. - alphas_cumprod) + self.v_posterior * betas\n",
        "        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n",
        "        self.register_buffer('posterior_variance', to_torch(posterior_variance))\n",
        "        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n",
        "        self.register_buffer('posterior_log_variance_clipped', to_torch(np.log(np.maximum(posterior_variance, 1e-20))))\n",
        "        self.register_buffer('posterior_mean_coef1', to_torch(\n",
        "            betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)))\n",
        "        self.register_buffer('posterior_mean_coef2', to_torch(\n",
        "            (1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)))\n",
        "\n",
        "        if self.parameterization == \"eps\":\n",
        "            lvlb_weights = self.betas ** 2 / (\n",
        "                        2 * self.posterior_variance * to_torch(alphas) * (1 - self.alphas_cumprod))\n",
        "        elif self.parameterization == \"x0\":\n",
        "            lvlb_weights = 0.5 * np.sqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))\n",
        "        else:\n",
        "            raise NotImplementedError(\"mu not supported\")\n",
        "\n",
        "        lvlb_weights[0] = lvlb_weights[1]\n",
        "        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n",
        "        assert not torch.isnan(self.lvlb_weights).all()\n",
        "\n",
        "    @contextmanager\n",
        "    def ema_scope(self, context=None):\n",
        "        if self.use_ema:\n",
        "            self.model_ema.store(self.model.parameters())\n",
        "            self.model_ema.copy_to(self.model)\n",
        "            if context is not None:\n",
        "                print(f\"{context}: Switched to EMA weights\")\n",
        "        try:\n",
        "            yield None\n",
        "        finally:\n",
        "            if self.use_ema:\n",
        "                self.model_ema.restore(self.model.parameters())\n",
        "                if context is not None:\n",
        "                    print(f\"{context}: Restored training weights\")\n",
        "\n",
        "    def init_from_ckpt(self, path, ignore_keys=list(), only_model=False):\n",
        "        sd = torch.load(path, map_location=\"cpu\")\n",
        "        if \"state_dict\" in list(sd.keys()):\n",
        "            sd = sd[\"state_dict\"]\n",
        "        keys = list(sd.keys())\n",
        "        for k in keys:\n",
        "            for ik in ignore_keys:\n",
        "                if k.startswith(ik):\n",
        "                    print(\"Deleting key {} from state_dict.\".format(k))\n",
        "                    del sd[k]\n",
        "        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n",
        "            sd, strict=False)\n",
        "        print(f\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\")\n",
        "        if len(missing) > 0:\n",
        "            print(f\"Missing Keys: {missing}\")\n",
        "        if len(unexpected) > 0:\n",
        "            print(f\"Unexpected Keys: {unexpected}\")\n",
        "\n",
        "\n",
        "    def predict_start_from_noise(self, x_t, t, noise):\n",
        "        return (\n",
        "                extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n",
        "                extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n",
        "        )\n",
        "\n",
        "\n",
        "    def q_sample(self, x_start, t, noise=None):\n",
        "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
        "        return (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n",
        "                extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise)\n",
        "\n",
        "    def get_loss(self, pred, target, mean=True):\n",
        "        if self.loss_type == 'l1':\n",
        "            loss = (target - pred).abs()\n",
        "            if mean:\n",
        "                loss = loss.mean()\n",
        "        elif self.loss_type == 'l2':\n",
        "            if mean:\n",
        "                loss = torch.nn.functional.mse_loss(target, pred)\n",
        "            else:\n",
        "                loss = torch.nn.functional.mse_loss(target, pred, reduction='none')\n",
        "        else:\n",
        "            raise NotImplementedError(\"unknown loss type '{loss_type}'\")\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def p_losses(self, x_start, t, noise=None):\n",
        "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
        "        x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n",
        "        model_out = self.model(x_noisy, t)\n",
        "\n",
        "        loss_dict = {}\n",
        "        if self.parameterization == \"eps\":\n",
        "            target = noise\n",
        "        elif self.parameterization == \"x0\":\n",
        "            target = x_start\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Paramterization {self.parameterization} not yet supported\")\n",
        "\n",
        "        loss = self.get_loss(model_out, target, mean=False).mean(dim=[1, 2, 3])\n",
        "\n",
        "        log_prefix = 'train' if self.training else 'val'\n",
        "\n",
        "        loss_dict.update({f'{log_prefix}/loss_simple': loss.mean()})\n",
        "        loss_simple = loss.mean() * self.l_simple_weight\n",
        "\n",
        "        loss_vlb = (self.lvlb_weights[t] * loss).mean()\n",
        "        loss_dict.update({f'{log_prefix}/loss_vlb': loss_vlb})\n",
        "\n",
        "        loss = loss_simple + self.original_elbo_weight * loss_vlb\n",
        "\n",
        "        loss_dict.update({f'{log_prefix}/loss': loss})\n",
        "\n",
        "        return loss, loss_dict\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "\n",
        "        t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=self.device).long()\n",
        "        return self.p_losses(x, t, *args, **kwargs)\n",
        "\n",
        "    def get_input(self, batch, k):\n",
        "        x = batch[k]\n",
        "        if len(x.shape) == 3:\n",
        "            x = x[..., None]\n",
        "        x = rearrange(x, 'b h w c -> b c h w')\n",
        "        x = x.to(memory_format=torch.contiguous_format).float()\n",
        "        return x\n",
        "\n",
        "    def shared_step(self, batch):\n",
        "        x = self.get_input(batch, self.first_stage_key)\n",
        "        loss, loss_dict = self(x)\n",
        "        return loss, loss_dict\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, loss_dict = self.shared_step(batch)\n",
        "\n",
        "        self.log_dict(loss_dict, prog_bar=True,\n",
        "                      logger=True, on_step=True, on_epoch=True)\n",
        "\n",
        "        self.log(\"global_step\", self.global_step,\n",
        "                 prog_bar=True, logger=True, on_step=True, on_epoch=False)\n",
        "\n",
        "        if self.use_scheduler:\n",
        "            lr = self.optimizers().param_groups[0]['lr']\n",
        "            self.log('lr_abs', lr, prog_bar=True, logger=True, on_step=True, on_epoch=False)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        _, loss_dict_no_ema = self.shared_step(batch)\n",
        "        with self.ema_scope():\n",
        "            _, loss_dict_ema = self.shared_step(batch)\n",
        "            loss_dict_ema = {key + '_ema': loss_dict_ema[key] for key in loss_dict_ema}\n",
        "        self.log_dict(loss_dict_no_ema, prog_bar=False, logger=True, on_step=False, on_epoch=True)\n",
        "        self.log_dict(loss_dict_ema, prog_bar=False, logger=True, on_step=False, on_epoch=True)\n",
        "\n",
        "    def on_train_batch_end(self, *args, **kwargs):\n",
        "        if self.use_ema:\n",
        "            self.model_ema(self.model)\n",
        "\n",
        "    def _get_rows_from_list(self, samples):\n",
        "        n_imgs_per_row = len(samples)\n",
        "        denoise_grid = rearrange(samples, 'n b c h w -> b n c h w')\n",
        "        denoise_grid = rearrange(denoise_grid, 'b n c h w -> (b n) c h w')\n",
        "        denoise_grid = make_grid(denoise_grid, nrow=n_imgs_per_row)\n",
        "        return denoise_grid\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        lr = self.learning_rate\n",
        "        params = list(self.model.parameters())\n",
        "        if self.learn_logvar:\n",
        "            params = params + [self.logvar]\n",
        "        opt = torch.optim.AdamW(params, lr=lr)\n",
        "        return opt"
      ],
      "metadata": {
        "id": "OGkoE27dIIyL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LatentDiffusion(DDPM):\n",
        "    \"\"\"main class\"\"\"\n",
        "    def __init__(self,\n",
        "                 first_stage_config,\n",
        "                 cond_stage_config,\n",
        "                 num_timesteps_cond=None,\n",
        "                 cond_stage_key=\"image\",\n",
        "                 cond_stage_trainable=False,\n",
        "                 concat_mode=True,\n",
        "                 cond_stage_forward=None,\n",
        "                 conditioning_key=None,\n",
        "                 scale_factor=1.0,\n",
        "                 scale_by_std=False,\n",
        "                 *args, **kwargs):\n",
        "\n",
        "        self.num_timesteps_cond = default(num_timesteps_cond, 1)\n",
        "        self.scale_by_std = scale_by_std\n",
        "        assert self.num_timesteps_cond <= kwargs['timesteps']\n",
        "\n",
        "        if conditioning_key is None:\n",
        "            conditioning_key = 'concat' if concat_mode else 'crossattn'\n",
        "        if cond_stage_config == '__is_unconditional__':\n",
        "            conditioning_key = None\n",
        "        ckpt_path = kwargs.pop(\"ckpt_path\", None)\n",
        "        ignore_keys = kwargs.pop(\"ignore_keys\", [])\n",
        "        super().__init__(conditioning_key=conditioning_key, *args, **kwargs)\n",
        "        self.concat_mode = concat_mode\n",
        "        self.cond_stage_trainable = cond_stage_trainable\n",
        "        self.cond_stage_key = cond_stage_key\n",
        "        try:\n",
        "            self.num_downs = len(first_stage_config.params.ddconfig.ch_mult) - 1\n",
        "        except:\n",
        "            self.num_downs = 0\n",
        "        if not scale_by_std:\n",
        "            self.scale_factor = scale_factor\n",
        "        else:\n",
        "            self.register_buffer('scale_factor', torch.tensor(scale_factor))\n",
        "        self.instantiate_first_stage(first_stage_config)\n",
        "        self.instantiate_cond_stage(cond_stage_config)\n",
        "        self.cond_stage_forward = cond_stage_forward\n",
        "        self.clip_denoised = False\n",
        "        self.bbox_tokenizer = None\n",
        "\n",
        "        self.restarted_from_ckpt = False\n",
        "        if ckpt_path is not None:\n",
        "            self.init_from_ckpt(ckpt_path, ignore_keys)\n",
        "            self.restarted_from_ckpt = True\n",
        "\n",
        "    def make_cond_schedule(self, ):\n",
        "        self.cond_ids = torch.full(size=(self.num_timesteps,), fill_value=self.num_timesteps - 1, dtype=torch.long)\n",
        "        ids = torch.round(torch.linspace(0, self.num_timesteps - 1, self.num_timesteps_cond)).long()\n",
        "        self.cond_ids[:self.num_timesteps_cond] = ids\n",
        "\n",
        "    @rank_zero_only\n",
        "    @torch.no_grad()\n",
        "    def on_train_batch_start(self, batch, batch_idx, dataloader_idx):\n",
        "        # only for very first batch\n",
        "        if self.scale_by_std and self.current_epoch == 0 and self.global_step == 0 and batch_idx == 0 and not self.restarted_from_ckpt:\n",
        "            assert self.scale_factor == 1., 'rather not use custom rescaling and std-rescaling simultaneously'\n",
        "            # set rescale weight to 1./std of encodings\n",
        "            print(\"### USING STD-RESCALING ###\")\n",
        "            x = super().get_input(batch, self.first_stage_key)\n",
        "            x = x.to(self.device)\n",
        "            encoder_posterior = self.encode_first_stage(x)\n",
        "            z = self.get_first_stage_encoding(encoder_posterior).detach()\n",
        "            del self.scale_factor\n",
        "            self.register_buffer('scale_factor', 1. / z.flatten().std())\n",
        "            print(f\"setting self.scale_factor to {self.scale_factor}\")\n",
        "            print(\"### USING STD-RESCALING ###\")\n",
        "\n",
        "    def register_schedule(self,\n",
        "                          given_betas=None, beta_schedule=\"linear\", timesteps=1000,\n",
        "                          linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n",
        "        super().register_schedule(given_betas, beta_schedule, timesteps, linear_start, linear_end, cosine_s)\n",
        "\n",
        "        self.shorten_cond_schedule = self.num_timesteps_cond > 1\n",
        "        if self.shorten_cond_schedule:\n",
        "            self.make_cond_schedule()\n",
        "\n",
        "    def instantiate_first_stage(self, config):\n",
        "        model = instantiate_from_config(config)\n",
        "        self.first_stage_model = model.eval()\n",
        "        self.first_stage_model.train = disabled_train\n",
        "        for param in self.first_stage_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def instantiate_cond_stage(self, config):\n",
        "        if not self.cond_stage_trainable:\n",
        "            if config == \"__is_first_stage__\":\n",
        "                print(\"Using first stage also as cond stage.\")\n",
        "                self.cond_stage_model = self.first_stage_model\n",
        "            elif config == \"__is_unconditional__\":\n",
        "                print(f\"Training {self.__class__.__name__} as an unconditional model.\")\n",
        "                self.cond_stage_model = None\n",
        "\n",
        "            else:\n",
        "                model = instantiate_from_config(config)\n",
        "                self.cond_stage_model = model.eval()\n",
        "                self.cond_stage_model.train = disabled_train\n",
        "                for param in self.cond_stage_model.parameters():\n",
        "                    param.requires_grad = False\n",
        "        else:\n",
        "            assert config != '__is_first_stage__'\n",
        "            assert config != '__is_unconditional__'\n",
        "            model = instantiate_from_config(config)\n",
        "            self.cond_stage_model = model\n",
        "\n",
        "\n",
        "    def get_first_stage_encoding(self, encoder_posterior):\n",
        "        if isinstance(encoder_posterior, DiagonalGaussianDistribution):\n",
        "            z = encoder_posterior.sample()\n",
        "        elif isinstance(encoder_posterior, torch.Tensor):\n",
        "            z = encoder_posterior\n",
        "        else:\n",
        "            raise NotImplementedError(f\"encoder_posterior of type '{type(encoder_posterior)}' not yet implemented\")\n",
        "        return self.scale_factor * z\n",
        "\n",
        "    def get_learned_conditioning(self, c):\n",
        "        if self.cond_stage_forward is None:\n",
        "            if hasattr(self.cond_stage_model, 'encode') and callable(self.cond_stage_model.encode):\n",
        "                c = self.cond_stage_model.encode(c)\n",
        "                if isinstance(c, DiagonalGaussianDistribution):\n",
        "                    c = c.mode()\n",
        "            else:\n",
        "                c = self.cond_stage_model(c)\n",
        "        else:\n",
        "            assert hasattr(self.cond_stage_model, self.cond_stage_forward)\n",
        "            c = getattr(self.cond_stage_model, self.cond_stage_forward)(c)\n",
        "        return c\n",
        "\n",
        "    def meshgrid(self, h, w):\n",
        "        y = torch.arange(0, h).view(h, 1, 1).repeat(1, w, 1)\n",
        "        x = torch.arange(0, w).view(1, w, 1).repeat(h, 1, 1)\n",
        "\n",
        "        arr = torch.cat([y, x], dim=-1)\n",
        "        return arr\n",
        "\n",
        "    def delta_border(self, h, w):\n",
        "        \"\"\"\n",
        "        :param h: height\n",
        "        :param w: width\n",
        "        :return: normalized distance to image border,\n",
        "         wtith min distance = 0 at border and max dist = 0.5 at image center\n",
        "        \"\"\"\n",
        "        lower_right_corner = torch.tensor([h - 1, w - 1]).view(1, 1, 2)\n",
        "        arr = self.meshgrid(h, w) / lower_right_corner\n",
        "        dist_left_up = torch.min(arr, dim=-1, keepdims=True)[0]\n",
        "        dist_right_down = torch.min(1 - arr, dim=-1, keepdims=True)[0]\n",
        "        edge_dist = torch.min(torch.cat([dist_left_up, dist_right_down], dim=-1), dim=-1)[0]\n",
        "        return edge_dist\n",
        "\n",
        "    def get_weighting(self, h, w, Ly, Lx, device):\n",
        "        weighting = self.delta_border(h, w)\n",
        "        weighting = torch.clip(weighting, self.split_input_params[\"clip_min_weight\"],\n",
        "                               self.split_input_params[\"clip_max_weight\"], )\n",
        "        weighting = weighting.view(1, h * w, 1).repeat(1, 1, Ly * Lx).to(device)\n",
        "\n",
        "        if self.split_input_params[\"tie_braker\"]:\n",
        "            L_weighting = self.delta_border(Ly, Lx)\n",
        "            L_weighting = torch.clip(L_weighting,\n",
        "                                     self.split_input_params[\"clip_min_tie_weight\"],\n",
        "                                     self.split_input_params[\"clip_max_tie_weight\"])\n",
        "\n",
        "            L_weighting = L_weighting.view(1, 1, Ly * Lx).to(device)\n",
        "            weighting = weighting * L_weighting\n",
        "        return weighting\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def get_input(self, batch, k, return_first_stage_outputs=False, force_c_encode=False,\n",
        "                  cond_key=None, return_original_cond=False, bs=None):\n",
        "        x = super().get_input(batch, k)\n",
        "        if bs is not None:\n",
        "            x = x[:bs]\n",
        "        x = x.to(self.device)\n",
        "        encoder_posterior = self.encode_first_stage(x)\n",
        "        z = self.get_first_stage_encoding(encoder_posterior).detach()\n",
        "\n",
        "        c = None\n",
        "        xc = None\n",
        "        if self.use_positional_encodings:\n",
        "            pos_x, pos_y = self.compute_latent_shifts(batch)\n",
        "            c = {'pos_x': pos_x, 'pos_y': pos_y}\n",
        "        out = [z, c]\n",
        "        if return_first_stage_outputs:\n",
        "            xrec = self.decode_first_stage(z)\n",
        "            out.extend([x, xrec])\n",
        "        if return_original_cond:\n",
        "            out.append(xc)\n",
        "        return out\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def decode_first_stage(self, z, predict_cids=False, force_not_quantize=False):\n",
        "        z = 1. / self.scale_factor * z\n",
        "        return self.first_stage_model.decode(z)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def encode_first_stage(self, x):\n",
        "        return self.first_stage_model.encode(x)\n",
        "\n",
        "    def shared_step(self, batch, **kwargs):\n",
        "        x, c = self.get_input(batch, self.first_stage_key)\n",
        "        loss = self(x, c)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x, c, *args, **kwargs):\n",
        "        t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=self.device).long()\n",
        "        if self.model.conditioning_key is not None:\n",
        "            assert c is not None\n",
        "            if self.cond_stage_trainable:\n",
        "                c = self.get_learned_conditioning(c)\n",
        "            if self.shorten_cond_schedule:\n",
        "                tc = self.cond_ids[t].to(self.device)\n",
        "                c = self.q_sample(x_start=c, t=tc, noise=torch.randn_like(c.float()))\n",
        "        return self.p_losses(x, c, t, *args, **kwargs)\n",
        "\n",
        "\n",
        "    def apply_model(self, x_noisy, t, cond, return_ids=False):\n",
        "        x_recon = self.model(x_noisy, t, **cond)\n",
        "        return x_recon\n",
        "\n",
        "    def _predict_eps_from_xstart(self, x_t, t, pred_xstart):\n",
        "        return (extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - pred_xstart) / \\\n",
        "               extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n",
        "\n",
        "    def _prior_bpd(self, x_start):\n",
        "\n",
        "        batch_size = x_start.shape[0]\n",
        "        t = torch.tensor([self.num_timesteps - 1] * batch_size, device=x_start.device)\n",
        "        qt_mean, _, qt_log_variance = self.q_mean_variance(x_start, t)\n",
        "        kl_prior = normal_kl(mean1=qt_mean, logvar1=qt_log_variance, mean2=0.0, logvar2=0.0)\n",
        "        return mean_flat(kl_prior) / np.log(2.0)\n",
        "\n",
        "    def p_losses(self, x_start, cond, t, noise=None):\n",
        "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
        "        x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n",
        "        model_output = self.apply_model(x_noisy, t, cond)\n",
        "\n",
        "        loss_dict = {}\n",
        "        prefix = 'train' if self.training else 'val'\n",
        "\n",
        "        if self.parameterization == \"x0\":\n",
        "            target = x_start\n",
        "        elif self.parameterization == \"eps\":\n",
        "            target = noise\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "\n",
        "        loss_simple = self.get_loss(model_output, target, mean=False).mean([1, 2, 3])\n",
        "        loss_dict.update({f'{prefix}/loss_simple': loss_simple.mean()})\n",
        "\n",
        "        logvar_t = self.logvar[t].to(self.device)\n",
        "        loss = loss_simple / torch.exp(logvar_t) + logvar_t\n",
        "\n",
        "        if self.learn_logvar:\n",
        "            loss_dict.update({f'{prefix}/loss_gamma': loss.mean()})\n",
        "            loss_dict.update({'logvar': self.logvar.data.mean()})\n",
        "\n",
        "        loss = self.l_simple_weight * loss.mean()\n",
        "\n",
        "        loss_vlb = self.get_loss(model_output, target, mean=False).mean(dim=(1, 2, 3))\n",
        "        loss_vlb = (self.lvlb_weights[t] * loss_vlb).mean()\n",
        "        loss_dict.update({f'{prefix}/loss_vlb': loss_vlb})\n",
        "        loss += (self.original_elbo_weight * loss_vlb)\n",
        "        loss_dict.update({f'{prefix}/loss': loss})\n",
        "\n",
        "        return loss, loss_dict\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        lr = self.learning_rate\n",
        "        params = list(self.model.parameters())\n",
        "        if self.cond_stage_trainable:\n",
        "            print(f\"{self.__class__.__name__}: Also optimizing conditioner params!\")\n",
        "            params = params + list(self.cond_stage_model.parameters())\n",
        "        if self.learn_logvar:\n",
        "            print('Diffusion model optimizing logvar')\n",
        "            params.append(self.logvar)\n",
        "        opt = torch.optim.AdamW(params, lr=lr)\n",
        "        if self.use_scheduler:\n",
        "            assert 'target' in self.scheduler_config\n",
        "            scheduler = instantiate_from_config(self.scheduler_config)\n",
        "\n",
        "            print(\"Setting up LambdaLR scheduler...\")\n",
        "            scheduler = [\n",
        "                {\n",
        "                    'scheduler': LambdaLR(opt, lr_lambda=scheduler.schedule),\n",
        "                    'interval': 'step',\n",
        "                    'frequency': 1\n",
        "                }]\n",
        "            return [opt], scheduler\n",
        "        return opt\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def to_rgb(self, x):\n",
        "        x = x.float()\n",
        "        if not hasattr(self, \"colorize\"):\n",
        "            self.colorize = torch.randn(3, x.shape[1], 1, 1).to(x)\n",
        "        x = nn.functional.conv2d(x, weight=self.colorize)\n",
        "        x = 2. * (x - x.min()) / (x.max() - x.min()) - 1.\n",
        "        return x"
      ],
      "metadata": {
        "id": "ASo1wRsgFJay"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from omegaconf import OmegaConf\n",
        "config = OmegaConf.load('configs/latent-diffusion/celebahq-ldm-vq-4.yaml')\n",
        "config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTx53LVgWfhR",
        "outputId": "aad793b3-4bf3-405e-8d03-ede56235e929"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model': {'base_learning_rate': 2e-06, 'target': 'ldm.models.diffusion.ddpm.LatentDiffusion', 'params': {'linear_start': 0.0015, 'linear_end': 0.0195, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'image', 'image_size': 64, 'channels': 3, 'monitor': 'val/loss_simple_ema', 'unet_config': {'target': 'ldm.modules.diffusionmodules.openaimodel.UNetModel', 'params': {'image_size': 64, 'in_channels': 3, 'out_channels': 3, 'model_channels': 224, 'attention_resolutions': [8, 4, 2], 'num_res_blocks': 2, 'channel_mult': [1, 2, 3, 4], 'num_head_channels': 32}}, 'first_stage_config': {'target': 'ldm.models.autoencoder.VQModelInterface', 'params': {'embed_dim': 3, 'n_embed': 8192, 'ckpt_path': 'models/first_stage_models/vq-f4/model.ckpt', 'ddconfig': {'double_z': False, 'z_channels': 3, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}}, 'cond_stage_config': '__is_unconditional__'}}, 'data': {'target': 'main.DataModuleFromConfig', 'params': {'batch_size': 48, 'num_workers': 5, 'wrap': False, 'train': {'target': 'taming.data.faceshq.CelebAHQTrain', 'params': {'size': 256}}, 'validation': {'target': 'taming.data.faceshq.CelebAHQValidation', 'params': {'size': 256}}}}, 'lightning': {'callbacks': {'image_logger': {'target': 'main.ImageLogger', 'params': {'batch_frequency': 5000, 'max_images': 8, 'increase_log_steps': False}}}, 'trainer': {'benchmark': True}}}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_stage_config = config.model.params.first_stage_config\n",
        "cond_stage_config = config.model.params.cond_stage_config\n",
        "unet_config = config.model.params.unet_config"
      ],
      "metadata": {
        "id": "ZXgTsCCBW3IS"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LatentDiffusion(first_stage_config,\n",
        "                        cond_stage_config,\n",
        "                        num_timesteps_cond=None,\n",
        "                        cond_stage_key=\"image\",\n",
        "                        cond_stage_trainable=False,\n",
        "                        concat_mode=True,\n",
        "                        cond_stage_forward=None,\n",
        "                        conditioning_key=None,\n",
        "                        scale_factor=1.0,\n",
        "                        scale_by_std=False,\n",
        "                        timesteps= 1000,\n",
        "                        unet_config = unet_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYRjNK8aWGpY",
        "outputId": "2c9d7219-dd42-4b42-96c9-042835ef504c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LatentDiffusion: Running in eps-prediction mode\n",
            "DiffusionWrapper has 274.06 M params.\n",
            "Keeping EMAs of 370.\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "Restored from models/first_stage_models/vq-f4/model.ckpt with 0 missing and 55 unexpected keys\n",
            "Training LatentDiffusion as an unconditional model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import abstractmethod\n",
        "from functools import partial\n",
        "import math\n",
        "from typing import Iterable\n",
        "import numpy as np\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from ldm.modules.diffusionmodules.util import (checkpoint,conv_nd,linear,avg_pool_nd,zero_module,normalization,timestep_embedding)\n",
        "from ldm.modules.attention import SpatialTransformer"
      ],
      "metadata": {
        "id": "omirXQqUssgN"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TimestepBlock(nn.Module):\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, x, emb):\n",
        "        \"\"\"\n",
        "        Apply the module to `x` given `emb` timestep embeddings.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "class TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n",
        "\n",
        "    def forward(self, x, emb, context=None):\n",
        "        for layer in self:\n",
        "            if isinstance(layer, TimestepBlock):\n",
        "                x = layer(x, emb)\n",
        "            elif isinstance(layer, SpatialTransformer):\n",
        "                x = layer(x, context)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "\n",
        "    def __init__(self, channels, use_conv, dims=2, out_channels=None, padding=1):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.out_channels = out_channels or channels\n",
        "        self.use_conv = use_conv\n",
        "        self.dims = dims\n",
        "        if use_conv:\n",
        "            self.conv = conv_nd(dims, self.channels, self.out_channels, 3, padding=padding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.shape[1] == self.channels\n",
        "        if self.dims == 3:\n",
        "            x = F.interpolate(\n",
        "                x, (x.shape[2], x.shape[3] * 2, x.shape[4] * 2), mode=\"nearest\"\n",
        "            )\n",
        "        else:\n",
        "            x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
        "        if self.use_conv:\n",
        "            x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "\n",
        "    def __init__(self, channels, use_conv, dims=2, out_channels=None,padding=1):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.out_channels = out_channels or channels\n",
        "        self.use_conv = use_conv\n",
        "        self.dims = dims\n",
        "        stride = 2 if dims != 3 else (1, 2, 2)\n",
        "        if use_conv:\n",
        "            self.op = conv_nd(\n",
        "                dims, self.channels, self.out_channels, 3, stride=stride, padding=padding\n",
        "            )\n",
        "        else:\n",
        "            assert self.channels == self.out_channels\n",
        "            self.op = avg_pool_nd(dims, kernel_size=stride, stride=stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.shape[1] == self.channels\n",
        "        return self.op(x)"
      ],
      "metadata": {
        "id": "4Hm_yhexs1iQ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResBlock(TimestepBlock):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels,\n",
        "        emb_channels,\n",
        "        dropout,\n",
        "        out_channels=None,\n",
        "        use_conv=False,\n",
        "        use_scale_shift_norm=False,\n",
        "        dims=2,\n",
        "        use_checkpoint=False,\n",
        "        up=False,\n",
        "        down=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.emb_channels = emb_channels\n",
        "        self.dropout = dropout\n",
        "        self.out_channels = out_channels or channels\n",
        "        self.use_conv = use_conv\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "        self.use_scale_shift_norm = use_scale_shift_norm\n",
        "\n",
        "        self.in_layers = nn.Sequential(\n",
        "            normalization(channels),\n",
        "            nn.SiLU(),\n",
        "            conv_nd(dims, channels, self.out_channels, 3, padding=1),\n",
        "        )\n",
        "\n",
        "        self.updown = up or down\n",
        "\n",
        "        if up:\n",
        "            self.h_upd = Upsample(channels, False, dims)\n",
        "            self.x_upd = Upsample(channels, False, dims)\n",
        "        elif down:\n",
        "            self.h_upd = Downsample(channels, False, dims)\n",
        "            self.x_upd = Downsample(channels, False, dims)\n",
        "        else:\n",
        "            self.h_upd = self.x_upd = nn.Identity()\n",
        "\n",
        "        self.emb_layers = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            linear(\n",
        "                emb_channels,\n",
        "                2 * self.out_channels if use_scale_shift_norm else self.out_channels,\n",
        "            ),\n",
        "        )\n",
        "        self.out_layers = nn.Sequential(\n",
        "            normalization(self.out_channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            zero_module(\n",
        "                conv_nd(dims, self.out_channels, self.out_channels, 3, padding=1)\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        if self.out_channels == channels:\n",
        "            self.skip_connection = nn.Identity()\n",
        "        elif use_conv:\n",
        "            self.skip_connection = conv_nd(\n",
        "                dims, channels, self.out_channels, 3, padding=1\n",
        "            )\n",
        "        else:\n",
        "            self.skip_connection = conv_nd(dims, channels, self.out_channels, 1)\n",
        "\n",
        "    def forward(self, x, emb):\n",
        "\n",
        "        return checkpoint(\n",
        "            self._forward, (x, emb), self.parameters(), self.use_checkpoint\n",
        "        )\n",
        "\n",
        "\n",
        "    def _forward(self, x, emb):\n",
        "        if self.updown:\n",
        "            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n",
        "            h = in_rest(x)\n",
        "            h = self.h_upd(h)\n",
        "            x = self.x_upd(x)\n",
        "            h = in_conv(h)\n",
        "        else:\n",
        "            h = self.in_layers(x)\n",
        "        emb_out = self.emb_layers(emb).type(h.dtype)\n",
        "        while len(emb_out.shape) < len(h.shape):\n",
        "            emb_out = emb_out[..., None]\n",
        "        if self.use_scale_shift_norm:\n",
        "            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n",
        "            scale, shift = th.chunk(emb_out, 2, dim=1)\n",
        "            h = out_norm(h) * (1 + scale) + shift\n",
        "            h = out_rest(h)\n",
        "        else:\n",
        "            h = h + emb_out\n",
        "            h = self.out_layers(h)\n",
        "        return self.skip_connection(x) + h"
      ],
      "metadata": {
        "id": "sxte_OW2s7UF"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QKVAttentionLegacy(nn.Module):\n",
        "    \"\"\"\n",
        "    A module which performs QKV attention. Matches legacy QKVAttention + input/ouput heads shaping\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_heads):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "    def forward(self, qkv):\n",
        "        \"\"\"\n",
        "        Apply QKV attention.\n",
        "        :param qkv: an [N x (H * 3 * C) x T] tensor of Qs, Ks, and Vs.\n",
        "        :return: an [N x (H * C) x T] tensor after attention.\n",
        "        \"\"\"\n",
        "        bs, width, length = qkv.shape\n",
        "        assert width % (3 * self.n_heads) == 0\n",
        "        ch = width // (3 * self.n_heads)\n",
        "        q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(ch, dim=1)\n",
        "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
        "        weight = th.einsum(\n",
        "            \"bct,bcs->bts\", q * scale, k * scale\n",
        "        )\n",
        "        weight = th.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
        "        a = th.einsum(\"bts,bcs->bct\", weight, v)\n",
        "        return a.reshape(bs, -1, length)"
      ],
      "metadata": {
        "id": "rIpE70MjtCvm"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlock(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels,\n",
        "        num_heads=1,\n",
        "        num_head_channels=-1,\n",
        "        use_checkpoint=False,\n",
        "        use_new_attention_order=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        if num_head_channels == -1:\n",
        "            self.num_heads = num_heads\n",
        "        else:\n",
        "            assert (\n",
        "                channels % num_head_channels == 0\n",
        "            ), f\"q,k,v channels {channels} is not divisible by num_head_channels {num_head_channels}\"\n",
        "            self.num_heads = channels // num_head_channels\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "        self.norm = normalization(channels)\n",
        "        self.qkv = conv_nd(1, channels, channels * 3, 1)\n",
        "\n",
        "        self.attention = QKVAttentionLegacy(self.num_heads)\n",
        "\n",
        "        self.proj_out = zero_module(conv_nd(1, channels, channels, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return checkpoint(self._forward, (x,), self.parameters(), True)\n",
        "\n",
        "    def _forward(self, x):\n",
        "        b, c, *spatial = x.shape\n",
        "        x = x.reshape(b, c, -1)\n",
        "        qkv = self.qkv(self.norm(x))\n",
        "        h = self.attention(qkv)\n",
        "        h = self.proj_out(h)\n",
        "        return (x + h).reshape(b, c, *spatial)"
      ],
      "metadata": {
        "id": "fs9snJEEs-9i"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNetModel(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size,\n",
        "        in_channels,\n",
        "        model_channels,\n",
        "        out_channels,\n",
        "        num_res_blocks,\n",
        "        attention_resolutions,\n",
        "        dropout=0,\n",
        "        channel_mult=(1, 2, 4, 8),\n",
        "        conv_resample=True,\n",
        "        dims=2,\n",
        "        num_classes=None,\n",
        "        use_checkpoint=False,\n",
        "        use_fp16=False,\n",
        "        num_heads=-1,\n",
        "        num_head_channels=-1,\n",
        "        num_heads_upsample=-1,\n",
        "        use_scale_shift_norm=False,\n",
        "        resblock_updown=False,\n",
        "        use_new_attention_order=False,\n",
        "        use_spatial_transformer=False,\n",
        "        transformer_depth=1,\n",
        "        context_dim=None,\n",
        "        n_embed=None,\n",
        "        legacy=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if use_spatial_transformer:\n",
        "            assert context_dim is not None, 'Fool!! You forgot to include the dimension of your cross-attention conditioning...'\n",
        "\n",
        "        if context_dim is not None:\n",
        "            assert use_spatial_transformer, 'Fool!! You forgot to use the spatial transformer for your cross-attention conditioning...'\n",
        "            from omegaconf.listconfig import ListConfig\n",
        "            if type(context_dim) == ListConfig:\n",
        "                context_dim = list(context_dim)\n",
        "\n",
        "        if num_heads_upsample == -1:\n",
        "            num_heads_upsample = num_heads\n",
        "\n",
        "        if num_heads == -1:\n",
        "            assert num_head_channels != -1, 'Either num_heads or num_head_channels has to be set'\n",
        "\n",
        "        if num_head_channels == -1:\n",
        "            assert num_heads != -1, 'Either num_heads or num_head_channels has to be set'\n",
        "\n",
        "        self.image_size = image_size\n",
        "        self.in_channels = in_channels\n",
        "        self.model_channels = model_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.attention_resolutions = attention_resolutions\n",
        "        self.dropout = dropout\n",
        "        self.channel_mult = channel_mult\n",
        "        self.conv_resample = conv_resample\n",
        "        self.num_classes = num_classes\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "        self.dtype = th.float16 if use_fp16 else th.float32\n",
        "        self.num_heads = num_heads\n",
        "        self.num_head_channels = num_head_channels\n",
        "        self.num_heads_upsample = num_heads_upsample\n",
        "        self.predict_codebook_ids = n_embed is not None\n",
        "\n",
        "        time_embed_dim = model_channels * 4\n",
        "        self.time_embed = nn.Sequential(\n",
        "            linear(model_channels, time_embed_dim),\n",
        "            nn.SiLU(),\n",
        "            linear(time_embed_dim, time_embed_dim),\n",
        "        )\n",
        "\n",
        "        if self.num_classes is not None:\n",
        "            self.label_emb = nn.Embedding(num_classes, time_embed_dim)\n",
        "\n",
        "        self.input_blocks = nn.ModuleList(\n",
        "            [\n",
        "                TimestepEmbedSequential(\n",
        "                    conv_nd(dims, in_channels, model_channels, 3, padding=1)\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "        self._feature_size = model_channels\n",
        "        input_block_chans = [model_channels]\n",
        "        ch = model_channels\n",
        "        ds = 1\n",
        "        for level, mult in enumerate(channel_mult):\n",
        "            for _ in range(num_res_blocks):\n",
        "                layers = [\n",
        "                    ResBlock(\n",
        "                        ch,\n",
        "                        time_embed_dim,\n",
        "                        dropout,\n",
        "                        out_channels=mult * model_channels,\n",
        "                        dims=dims,\n",
        "                        use_checkpoint=use_checkpoint,\n",
        "                        use_scale_shift_norm=use_scale_shift_norm,\n",
        "                    )\n",
        "                ]\n",
        "                ch = mult * model_channels\n",
        "                if ds in attention_resolutions:\n",
        "                    if num_head_channels == -1:\n",
        "                        dim_head = ch // num_heads\n",
        "                    else:\n",
        "                        num_heads = ch // num_head_channels\n",
        "                        dim_head = num_head_channels\n",
        "                    if legacy:\n",
        "                        dim_head = ch // num_heads if use_spatial_transformer else num_head_channels\n",
        "                    layers.append(\n",
        "                        AttentionBlock(\n",
        "                            ch,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            num_heads=num_heads,\n",
        "                            num_head_channels=dim_head,\n",
        "                            use_new_attention_order=use_new_attention_order,\n",
        "                        ) if not use_spatial_transformer else SpatialTransformer(\n",
        "                            ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim\n",
        "                        )\n",
        "                    )\n",
        "                self.input_blocks.append(TimestepEmbedSequential(*layers))\n",
        "                self._feature_size += ch\n",
        "                input_block_chans.append(ch)\n",
        "            if level != len(channel_mult) - 1:\n",
        "                out_ch = ch\n",
        "                self.input_blocks.append(\n",
        "                    TimestepEmbedSequential(\n",
        "                        ResBlock(\n",
        "                            ch,\n",
        "                            time_embed_dim,\n",
        "                            dropout,\n",
        "                            out_channels=out_ch,\n",
        "                            dims=dims,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            use_scale_shift_norm=use_scale_shift_norm,\n",
        "                            down=True,\n",
        "                        )\n",
        "                        if resblock_updown\n",
        "                        else Downsample(\n",
        "                            ch, conv_resample, dims=dims, out_channels=out_ch\n",
        "                        )\n",
        "                    )\n",
        "                )\n",
        "                ch = out_ch\n",
        "                input_block_chans.append(ch)\n",
        "                ds *= 2\n",
        "                self._feature_size += ch\n",
        "\n",
        "        if num_head_channels == -1:\n",
        "            dim_head = ch // num_heads\n",
        "        else:\n",
        "            num_heads = ch // num_head_channels\n",
        "            dim_head = num_head_channels\n",
        "        if legacy:\n",
        "            dim_head = ch // num_heads if use_spatial_transformer else num_head_channels\n",
        "        self.middle_block = TimestepEmbedSequential(\n",
        "            ResBlock(\n",
        "                ch,\n",
        "                time_embed_dim,\n",
        "                dropout,\n",
        "                dims=dims,\n",
        "                use_checkpoint=use_checkpoint,\n",
        "                use_scale_shift_norm=use_scale_shift_norm,\n",
        "            ),\n",
        "            AttentionBlock(\n",
        "                ch,\n",
        "                use_checkpoint=use_checkpoint,\n",
        "                num_heads=num_heads,\n",
        "                num_head_channels=dim_head,\n",
        "                use_new_attention_order=use_new_attention_order,\n",
        "            ) if not use_spatial_transformer else SpatialTransformer(\n",
        "                            ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim\n",
        "                        ),\n",
        "            ResBlock(\n",
        "                ch,\n",
        "                time_embed_dim,\n",
        "                dropout,\n",
        "                dims=dims,\n",
        "                use_checkpoint=use_checkpoint,\n",
        "                use_scale_shift_norm=use_scale_shift_norm,\n",
        "            ),\n",
        "        )\n",
        "        self._feature_size += ch\n",
        "\n",
        "        self.output_blocks = nn.ModuleList([])\n",
        "        for level, mult in list(enumerate(channel_mult))[::-1]:\n",
        "            for i in range(num_res_blocks + 1):\n",
        "                ich = input_block_chans.pop()\n",
        "                layers = [\n",
        "                    ResBlock(\n",
        "                        ch + ich,\n",
        "                        time_embed_dim,\n",
        "                        dropout,\n",
        "                        out_channels=model_channels * mult,\n",
        "                        dims=dims,\n",
        "                        use_checkpoint=use_checkpoint,\n",
        "                        use_scale_shift_norm=use_scale_shift_norm,\n",
        "                    )\n",
        "                ]\n",
        "                ch = model_channels * mult\n",
        "                if ds in attention_resolutions:\n",
        "                    if num_head_channels == -1:\n",
        "                        dim_head = ch // num_heads\n",
        "                    else:\n",
        "                        num_heads = ch // num_head_channels\n",
        "                        dim_head = num_head_channels\n",
        "                    if legacy:\n",
        "                        dim_head = ch // num_heads if use_spatial_transformer else num_head_channels\n",
        "                    layers.append(\n",
        "                        AttentionBlock(\n",
        "                            ch,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            num_heads=num_heads_upsample,\n",
        "                            num_head_channels=dim_head,\n",
        "                            use_new_attention_order=use_new_attention_order,\n",
        "                        ) if not use_spatial_transformer else SpatialTransformer(\n",
        "                            ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim\n",
        "                        )\n",
        "                    )\n",
        "                if level and i == num_res_blocks:\n",
        "                    out_ch = ch\n",
        "                    layers.append(\n",
        "                        ResBlock(\n",
        "                            ch,\n",
        "                            time_embed_dim,\n",
        "                            dropout,\n",
        "                            out_channels=out_ch,\n",
        "                            dims=dims,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            use_scale_shift_norm=use_scale_shift_norm,\n",
        "                            up=True,\n",
        "                        )\n",
        "                        if resblock_updown\n",
        "                        else Upsample(ch, conv_resample, dims=dims, out_channels=out_ch)\n",
        "                    )\n",
        "                    ds //= 2\n",
        "                self.output_blocks.append(TimestepEmbedSequential(*layers))\n",
        "                self._feature_size += ch\n",
        "\n",
        "        self.out = nn.Sequential(\n",
        "            normalization(ch),\n",
        "            nn.SiLU(),\n",
        "            zero_module(conv_nd(dims, model_channels, out_channels, 3, padding=1)),\n",
        "        )\n",
        "        if self.predict_codebook_ids:\n",
        "            self.id_predictor = nn.Sequential(\n",
        "            normalization(ch),\n",
        "            conv_nd(dims, model_channels, n_embed, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, timesteps=None, context=None, y=None,**kwargs):\n",
        "\n",
        "        assert (y is not None) == (self.num_classes is not None), \"must specify y if and only if the model is class-conditional\"\n",
        "        hs = []\n",
        "        t_emb = timestep_embedding(timesteps, self.model_channels, repeat_only=False)\n",
        "        emb = self.time_embed(t_emb)\n",
        "\n",
        "        if self.num_classes is not None:\n",
        "            assert y.shape == (x.shape[0],)\n",
        "            emb = emb + self.label_emb(y)\n",
        "\n",
        "        h = x.type(self.dtype)\n",
        "        for module in self.input_blocks:\n",
        "            h = module(h, emb, context)\n",
        "            hs.append(h)\n",
        "        h = self.middle_block(h, emb, context)\n",
        "        for module in self.output_blocks:\n",
        "            h = th.cat([h, hs.pop()], dim=1)\n",
        "            h = module(h, emb, context)\n",
        "        h = h.type(x.dtype)\n",
        "        if self.predict_codebook_ids:\n",
        "            return self.id_predictor(h)\n",
        "        else:\n",
        "            return self.out(h)"
      ],
      "metadata": {
        "id": "nYaddmP8sn1p"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DiffusionWrapper(unet_config, None)"
      ],
      "metadata": {
        "id": "t4vXvgyftFIq"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_e3OD_-i3wDN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
